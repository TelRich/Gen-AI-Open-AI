{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "This section delves into the application of Transformer models for text summarization, which involves condensing lengthy documents into shorter summaries. Text summarization stands as one of the most intricate NLP tasks, demanding a combination of skills such as comprehending lengthy passages and producing coherent text that encapsulates the central themes of a document. Successful text summarization serves as a powerful tool that can streamline various business processes by alleviating the need for domain experts to meticulously review lengthy documents.\n",
    "\n",
    "This section also introduces the concept of training a bilingual Transformer model for summarizing customer reviews in both English and Spanish. By the end of this section, you will have developed a model capable of effectively summarizing customer reviews in both languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a multilingual corpus\n",
    "\n",
    "To create a bilingual text summarization model, we'll utilize the [Multilingual Amazon Reviews Corpus](https://huggingface.co/datasets/amazon_reviews_multi), a collection of Amazon product reviews in six languages. Traditionally employed for evaluating multilingual classifiers, this corpus proves valuable for our summarization task due to the inclusion of short titles alongside each review. These titles will serve as target summaries for our model. To initiate the process, we'll download the English and Spanish subsets from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "spanish_dataset = load_dataset(\"amazon_reviews_multi\", \"es\")\n",
    "english_dataset = load_dataset(\"amazon_reviews_multi\", \"en\")\n",
    "english_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multilingual Amazon Reviews Corpus contains 200,000 reviews for the training split in each language and 5,000 reviews for each of the validation and test splits. The relevant review information is stored in the `review_body` and `review_title` columns. To examine a few examples, we can create a simple function that retrieves a random sample from the training set using the techniques introduced in Chapter 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(dataset, num_samples=3, seed=42):\n",
    "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
    "    for example in sample:\n",
    "        print(f\"\\n'>> Title: {example['review_title']}'\")\n",
    "        print(f\"'>> Review: {example['review_body']}'\")\n",
    "\n",
    "\n",
    "show_samples(english_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet unveils the diverse range of opinions lurking within online reviews, showcasing a spectrum from ecstatic praise to scathing criticism. While the \"meh\" title leaves much to be desired, others appear to effectively capture the essence of their corresponding reviews. However, training a summarization model on all 400,000 reviews would be a marathon for a single GPU. Therefore, we'll focus our efforts on summarizing reviews within a specific product category. To discover potential domains, let's convert the `english_dataset` to a `pandas.DataFrame` and tabulate the review count for each product category. This will provide a roadmap for selecting the domain that best suits our summarization needs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset.set_format(\"pandas\")\n",
    "english_df = english_dataset[\"train\"][:]\n",
    "# Show counts for top 20 products\n",
    "english_df[\"product_category\"].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The popularity contest amongst English dataset products is fierce, with household items, clothing, and wireless electronics reigning supreme. But staying true to Amazon's roots, let's set our sights on the captivating world of books! After all, this is where the company's story began. We spot two promising categories - \"book\" and \"digital_ebook_purchase\" - ripe for summarization. To delve deeper into the bookish realm, let's filter both the English and Spanish datasets to include only these two categories. This focused approach will allow us to tailor the summarization model to the specific nuances of book reviews, ensuring optimal performance and delightful results!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_books(example):\n",
    "    return (\n",
    "        example[\"product_category\"] == \"book\"\n",
    "        or example[\"product_category\"] == \"digital_ebook_purchase\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before embarking on our bookish summarization journey, we need to ensure our tools are ready. While the function you provided will expertly filter both English and Spanish datasets for book-related reviews, a crucial step remains. We must transform the format of `english_dataset` back from `pandas` to `arrow`. This is because the summarization model expects data in the `arrow` format for optimal processing and efficiency.\n",
    "\n",
    "Think of it as switching gears – pandas is great for exploration and analysis, but arrow is the sleek, high-performance vehicle for feeding the model and generating summaries. With this format switch, we'll be ready to filter both datasets and dive into the fascinating world of book reviews with sharpened focus and confidence!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply the filter function, and as a sanity check let’s inspect a sample of reviews to see if they are indeed about books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_books = spanish_dataset.filter(filter_books)\n",
    "english_books = english_dataset.filter(filter_books)\n",
    "show_samples(english_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, the perils of data exploration! While the reviews seemed initially bookish, closer inspection reveals they encompass a broader range, including calendars and electronic applications like OneNote. This slight detour from our pure bookish focus shouldn't deter us, however. The domain still holds potential for training a summarization model.\n",
    "\n",
    "Before delving into suitable models, one final data preparation step remains: merging the English and Spanish reviews into a single `DatasetDict` object.  Datasets offers a convenient `concatenate_datasets()` function, aptly named for its ability to stack two `Dataset` objects like building blocks. To create our bilingual dataset, we'll iterate through each split (train, validation, test), concatenate the corresponding English and Spanish datasets, and shuffle the result. This ensures our model encounters both languages throughout its training journey, preventing it from favoring one over the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "books_dataset = DatasetDict()\n",
    "\n",
    "for split in english_books.keys():\n",
    "    books_dataset[split] = concatenate_datasets(\n",
    "        [english_books[split], spanish_books[split]]\n",
    "    )\n",
    "    books_dataset[split] = books_dataset[split].shuffle(seed=42)\n",
    "\n",
    "# Peek at a few examples\n",
    "show_samples(books_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the reviews and titles contain a blend of English and Spanish content. Checking the training data involves examining how words are distributed within both the reviews and their titles. This examination is crucial, particularly for tasks like summarization. In such tasks, brief summaries within the dataset could influence the model to generate similarly short summaries consisting of only a word or two. The visual representations below display these word distributions. Notably, the titles exhibit a significant inclination towards containing just 1-2 words. This imbalance might impact how the model generates summaries.\n",
    "\n",
    "![](2023-12-16-13-46-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this, we'll exclude instances with extremely short titles, allowing our model to generate more engaging summaries. Given the mix of English and Spanish texts, we can apply a basic approach to split the titles using whitespace. Then, employing the `Dataset.filter()` method, we can proceed in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_dataset = books_dataset.filter(lambda x: len(x[\"review_title\"].split()) > 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our corpus is ready, let's explore some potential Transformer models suitable for fine-tuning on this data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models for text summarization\n",
    "\n",
    "Summarizing text shares a key similarity with machine translation: both aim to condense information while preserving essential points. Like translation models, most Transformer-based summarizers rely on the encoder-decoder architecture, though alternatives like GPT-family models exist for specific use cases. Here's a table highlighting popular pre-trained models that can be fine-tuned for summarization tasks.\n",
    "\n",
    "\n",
    "| Transformer model | Description | Multilingual? |\n",
    "|-------------------|-------------|---------------|\n",
    "| GPT-2             | Although trained as an auto-regressive language model, you can make GPT-2 generate summaries by appending “TL;DR” at the end of the input text. | ❌ |\n",
    "| PEGASUS           | Uses a pretraining objective to predict masked sentences in multi-sentence texts. This pretraining objective is closer to summarization than vanilla language modeling and scores highly on popular benchmarks. | ❌ |\n",
    "| T5                | A universal Transformer architecture that formulates all tasks in a text-to-text framework; e.g., the input format for the model to summarize a document is summarize: ARTICLE. | ❌ |\n",
    "| mT5               | A multilingual version of T5, pretrained on the multilingual Common Crawl corpus (mC4), covering 101 languages. | ✅ |\n",
    "| BART              | A novel Transformer architecture with both an encoder and a decoder stack trained to reconstruct corrupted input that combines the pretraining schemes of BERT and GPT-2. | ❌ |\n",
    "| mBART-50          | A multilingual version of BART, pretrained on 50 languages. | ✅ |\n",
    "\n",
    "\n",
    "As illustrated in the table, most Transformer models designed for summarization (and generally in NLP tasks) are designed for a single language. While this is advantageous for \"high-resource\" languages like English or German, it presents limitations for the numerous other languages worldwide. Thankfully, a category of multilingual Transformer models, such as mT5 and mBART, steps in to address this issue. These models undergo pretraining in a unique manner: instead of training solely on one language corpus, they're trained simultaneously on texts from over 50 languages!\n",
    "\n",
    "Let's delve into mT5, an intriguing architecture based on T5 that operates within a text-to-text framework. In T5, each NLP task is framed using a prompt prefix, such as \"summarize:\", which conditions the model to tailor the generated text according to the prompt. This approach renders T5 incredibly versatile, enabling it to handle a multitude of tasks using a single model, as depicted in the figure below.\n",
    "\n",
    "![](2023-12-16-14-05-59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model selection process, we've opted for mT5 due to its multilingual capabilities and versatile nature akin to T5. Now, moving forward, our focus shifts to the crucial stage of data preparation for training. This step is pivotal as it lays the groundwork for the model to learn from our dataset effectively.\n",
    "\n",
    "Our next step involves tokenizing and encoding both the reviews and their corresponding titles. To kickstart this process, we'll initiate by loading the tokenizer linked to the pretrained model checkpoint. In this scenario, we'll opt for \"mt5-small\" as our checkpoint to ensure a manageable fine-tuning duration for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 In the initial phases of NLP projects, it's beneficial to train \"small\" models using a limited dataset. This approach facilitates quicker debugging and iteration towards establishing an end-to-end workflow. Once you've gained confidence in the outcomes, scaling up the model is straightforward—simply by switching to a different model checkpoint!\n",
    "\n",
    "Let’s test out the mT5 tokenizer on a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"I loved reading the Hunger Games!\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're presented with the familiar input_ids and attention_mask components, reminiscent of our initial encounters during the early fine-tuning experiments in Chapter 3. To unveil the nature of this tokenizer, let's employ the tokenizer's `convert_ids_to_tokens()` function to decode these input IDs and inspect the tokenizer's characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presence of the special Unicode character ▁ and the end-of-sequence token </s> indicates that the tokenizer in use is SentencePiece, which relies on the Unigram segmentation algorithm. Unigram proves particularly advantageous for multilingual corpora as it remains agnostic to accents, punctuation, and the absence of whitespace characters in certain languages like Japanese.\n",
    "\n",
    "Regarding tokenization for summarization, there's a crucial consideration: the labels, being textual data, might exceed the model's maximum context size. Hence, truncating both the reviews and their corresponding titles becomes essential to prevent overly lengthy inputs from being processed by the model. Fortunately, 🤗 Transformers' tokenizers offer a convenient `text_target` argument, allowing parallel tokenization of the labels and inputs. Below is an example demonstrating how inputs and targets are processed for mT5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 30\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"review_body\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"review_title\"], max_length=max_target_length, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by setting values for `max_input_length` and `max_target_length`, determining the maximum allowable lengths for the reviews and titles, respectively. Given that the review body tends to be longer than the title, we've scaled these values accordingly.\n",
    "\n",
    "The `preprocess_function()` allows straightforward tokenization of the entire corpus using the `Dataset.map()` function, a utility extensively utilized throughout this course. This function ensures that the reviews and titles are tokenized while considering the maximum lengths specified for both inputs (`max_input_length`) and targets (`max_target_length`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = books_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the corpus has undergone preprocessing, let's delve into several metrics commonly employed for summarization. As we'll discover, there isn't a one-size-fits-all metric for gauging the quality of machine-generated text.\n",
    "\n",
    "💡 It's worth noting that in the `Dataset.map()` function earlier, we utilized `batched=True`. This parameter encodes examples in batches, typically of size 1,000 by default, harnessing the multithreading capabilities of fast tokenizers within 🤗 Transformers. Whenever feasible, employing `batched=True` can optimize preprocessing and leverage the efficiency of tokenization processes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for text summarization\n",
    "\n",
    "When it comes to tasks like summarization or translation, measuring performance isn't as straightforward compared to most other tasks we've covered in this course. Consider a review such as \"I loved reading the Hunger Games\"; multiple valid summaries like \"I loved the Hunger Games\" or \"Hunger Games is a great read\" exist. It's evident that exact matching between the generated summary and the label isn't a suitable solution—humans themselves would struggle under such a metric due to individual writing styles.\n",
    "\n",
    "For summarization tasks, the ROUGE score (short for Recall-Oriented Understudy for Gisting Evaluation) is one of the most frequently used metrics. This metric's core concept involves comparing a generated summary against a set of reference summaries typically crafted by humans. \n",
    "\n",
    "#### Brief note on ROUGE Score\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used in natural language processing and specifically in evaluating the quality of text generated by machine learning models, especially in tasks like text summarization and machine translation.\n",
    "\n",
    "It's designed to measure the similarity between automatically generated summaries or translations and reference (human-created) summaries or translations. The metrics in the ROUGE family primarily focus on recall, evaluating how much of the information in the generated text overlaps with the reference text.\n",
    "\n",
    "ROUGE computes various scores, such as ROUGE-N, ROUGE-L, and ROUGE-W, among others. These scores capture different aspects of overlap between the generated and reference texts:\n",
    "\n",
    "- **ROUGE-N:** Measures the overlap of n-grams (sequences of n words) between the generated and reference texts. ROUGE-1 focuses on single words (unigrams), ROUGE-2 on pairs of words (bigrams), and so on.\n",
    "\n",
    "- **ROUGE-L:** Computes the longest common subsequence between the generated and reference texts, considering sentence-level overlap and accounting for the order of words.\n",
    "\n",
    "- **ROUGE-W:** Considers weighted LCS (Longest Common Subsequence) to give higher importance to consecutive matches.\n",
    "\n",
    "These scores help assess the quality of machine-generated text by quantifying how much content overlaps with the reference text. Higher ROUGE scores indicate better agreement between the generated and reference text, suggesting better quality in tasks like summarization or translation.\n",
    "\n",
    "For example, if a model generates a summary with a high ROUGE score, it implies that the summary captures important information present in the reference summary, indicating a better quality summary. It's an essential evaluation metric used in assessing the performance of models in text-related tasks.\n",
    "\n",
    "To illustrate this in detail, let's consider comparing the following two summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straightforward method to compare these summaries might involve counting the number of overlapping words, which in this case would be 6. However, this approach is somewhat rudimentary. Hence, the ROUGE metric relies on calculating precision and recall scores specifically for the overlap between the generated summary and the reference summaries.\n",
    "\n",
    "🙋 No need to worry if precision and recall are new concepts for you! We'll walk through explicit examples together to clarify these metrics. Typically encountered in classification tasks, precision and recall hold specific definitions in that context. If you're interested in understanding how precision and recall are defined in classification, exploring [guides](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) from libraries like scikit-learn can be quite helpful.\n",
    "\n",
    "In the context of ROUGE, recall gauges how much of the reference summary is captured by the generated one. If we're simply comparing words, recall can be calculated using the following formula:\n",
    "\n",
    "![](2023-12-16-14-38-24.png)\n",
    "\n",
    "For our previous straightforward example, applying this formula yields a perfect recall of \\( \\frac{6}{6} = 1 \\), indicating that all words in the reference summary have been produced by the model.\n",
    "\n",
    "However, consider a scenario where the generated summary is \"I really really loved reading the Hunger Games all night.\" Surprisingly, this also achieves perfect recall (6/6), yet it's arguably a worse summary due to its verbosity. To address such cases, we compute precision, which in the ROUGE context measures how relevant the generated summary is:\n",
    "\n",
    "![](2023-12-16-14-39-02.png)\n",
    "\n",
    "Applying this to our verbose summary results in a precision of 6/10 = 0.6, significantly worse than the precision of 6/7 = 0.86 obtained by the shorter one.\n",
    "\n",
    "In practice, both precision and recall are computed, and then the F1-score, the harmonic mean of precision and recall, is commonly reported. To facilitate this computation within 🤗 Datasets, we start by installing the rouge_score package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then loading the ROUGE metric as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage the `rouge_score.compute()` function from the `rouge_score` package we installed to calculate all the relevant metrics simultaneously. This function streamlines the computation of various ROUGE metrics, providing a comprehensive overview of the evaluation results in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary], references=[reference_summary]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That output does contain quite a bit of information. 🤗 Datasets performs computations not only for precision, recall, and F1-score but also generates confidence intervals for these metrics. You'll notice attributes such as low, mid, and high, which correspond to these confidence intervals.\n",
    "\n",
    "In addition, various ROUGE scores are computed based on different levels of text granularity when comparing the generated and reference summaries. For instance, the rouge1 variant represents the overlap of unigrams, essentially measuring the overlap of individual words as we discussed earlier.\n",
    "\n",
    "To verify this further, let's extract the mid value from our computed scores. This value encapsulates the central tendency of the ROUGE scores, providing a clearer understanding of the evaluation outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"rouge1\"].mid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those other ROUGE scores capture different aspects of summarization evaluation. \n",
    "\n",
    "- **Rouge2** assesses the overlap between bigrams, essentially capturing the similarity between pairs of words in the generated and reference summaries.\n",
    "- **RougeL** and **RougeLsum** focus on identifying the longest matching sequences of words between the generated and reference summaries. \n",
    "    - **RougeLsum** computes this metric over the entire summary, considering the longest common substrings in the entire summary.\n",
    "    - **RougeL**, on the other hand, is computed as the average over individual sentences, aiming to identify the longest common sequences of words within each sentence.\n",
    "\n",
    "Each of these ROUGE variants offers a distinct perspective on how well the generated summary aligns with the reference summaries, capturing nuances in the overlap and sequence matching at various text granularities.\n",
    "\n",
    "Before diving into tracking our model's performance using ROUGE scores, let's execute a fundamental yet crucial step in NLP: establishing a robust yet straightforward baseline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a strong baseline\n",
    "\n",
    "A prevalent baseline for text summarization involves extracting the initial three sentences of an article, often referred to as the lead-3 baseline. While tracking sentence boundaries using full stops might falter with acronyms like \"U.S.\" or \"U.N.,\" we'll opt for a more robust solution provided by the nltk library. To install the package, you can use pip with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then download the punctuation rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we'll import the sentence tokenizer from nltk and craft a straightforward function to extract the initial three sentences from a review. In text summarization, it's customary to delimit each summary with a newline character. Let's incorporate this practice and test it on a training example to ensure its functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "\n",
    "print(three_sentence_summary(books_dataset[\"train\"][1][\"review_body\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed by implementing a function that extracts these \"summaries\" from a dataset and computes the ROUGE scores for the baseline. This function will involve extracting the lead-3 summaries from the dataset and subsequently calculating the ROUGE scores to evaluate the baseline's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[\"review_body\"]]\n",
    "    return metric.compute(predictions=summaries, references=dataset[\"review_title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can compute the ROUGE scores over the validation set. Afterward, we can utilize Pandas to present these scores in a more organized and readable format, enhancing their clarity and comprehensibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "score = evaluate_baseline(books_dataset[\"validation\"], rouge_score)\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)\n",
    "rouge_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noticeable decrease in the rouge2 score likely indicates that the lead-3 baseline tends to be more verbose compared to concise review titles. With this solid baseline established, let's shift our focus towards fine-tuning mT5!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning mT5 with the Trainer API\n",
    "\n",
    "Fine-tuning a model for summarization follows a process similar to the other tasks we've explored. Initially, we start by loading the pretrained model from the `mt5-small` checkpoint. As summarization is a sequence-to-sequence task, we'll use the `AutoModelForSeq2SeqLM` class to load the model, allowing it to automatically fetch and cache the necessary weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsequent step involves logging in to the Hugging Face Hub. If you're executing this code in a notebook, you can utilize the following utility function to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which will display a widget where you can enter your credentials. Alternatively, you can run this command in your terminal and log in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute ROUGE scores during training, we'll require the generation of summaries. Fortunately, 🤗 Transformers offers specialized classes such as `Seq2SeqTrainingArguments` and `Seq2SeqTrainer` designed to handle this task seamlessly. To observe this in action, let's initiate the definition of hyperparameters and other essential arguments for our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-amazon-en-es\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the provided setup, the argument `predict_with_generate` has been configured to enable the generation of summaries during evaluation. This facilitates the computation of ROUGE scores for each epoch by utilizing the model's `generate()` method, employing an iterative token prediction process within the decoder, as discussed in Chapter 1.\n",
    "\n",
    "Several default hyperparameters have been adjusted, including the learning rate, number of epochs, and weight decay. Additionally, the `save_total_limit` option has been set to retain only up to 3 checkpoints during training to conserve storage space, considering that even the \"small\" version of mT5 utilizes around a GB of hard drive space.\n",
    "\n",
    "The `push_to_hub=True` argument enables pushing the trained model to the Hub after completion of training. This action creates a repository under your user profile, specified by `output_dir`. It's noteworthy that you can define the repository's name using the `hub_model_id` argument, especially when pushing to an organization. For instance, when pushing the model to the huggingface-course organization, we appended `hub_model_id=\"huggingface-course/mt5-finetuned-amazon-en-es\"` to Seq2SeqTrainingArguments.\n",
    "\n",
    "The subsequent step involves providing the trainer with a `compute_metrics()` function to evaluate the model during training. For summarization tasks, this process requires more than simply applying `rouge_score.compute()` on the model's predictions. Instead, we must decode the outputs and labels into text before computing the ROUGE scores. The function below accomplishes this task, utilizing the `sent_tokenize()` function from nltk to separate the summary sentences with newlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step involves defining a data collator specifically tailored for our sequence-to-sequence task. When working with an encoder-decoder Transformer model like mT5, a crucial aspect in batch preparation is the shifting of labels to the right by one during decoding. This shift is necessary to ensure that the decoder only receives the previous ground truth labels, preventing it from accessing current or future labels, which could lead to memorization by the model. This process aligns with how masked self-attention is applied to inputs in tasks like [causal language modeling](https://huggingface.co/course/chapter7/6).\n",
    "\n",
    "Fortunately, 🤗 Transformers offers a `DataCollatorForSeq2Seq` collator designed to handle dynamic padding for both inputs and labels. Instantiating this collator involves providing the tokenizer and model as arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing a small batch of examples to the collator, let's remove the columns containing strings because the collator won't be able to pad these elements appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    books_dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To align with the collator's expectations, which require a list of dictionaries where each dictionary represents a single example in the dataset, we'll need to format the data accordingly before passing it to the data collator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notable aspect here is the discrepancy in length between the first and second examples. As a result, the `input_ids` and `attention_mask` of the second example have been right-padded with a [PAD] token (ID: 0). Similarly, the `labels` have been padded with `-100s` to ensure these tokens are disregarded by the loss function. Additionally, a new `decoder_input_ids` field has been generated, shifting the labels to the right by inserting a [PAD] token as the first entry.\n",
    "\n",
    "With all these elements in place, we have everything essential for training! Now, the next step involves simply instantiating the trainer using the standard arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and launch our training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process, you'll notice the training loss decreasing while the ROUGE scores gradually increase with each epoch, reflecting the model's improving performance. Upon completion of training, you can access the final ROUGE scores by executing `Trainer.evaluate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained scores indicate that our model has significantly surpassed the performance of our lead-3 baseline — an excellent achievement! The final step involves pushing the model weights to the Hub, accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub(commit_message=\"Training complete\", tags=\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing this command will save the checkpoint and configuration files to the specified output directory before uploading all files to the Hub. By utilizing the tags argument, we ensure that the Hub displays a summarization pipeline widget instead of the default text generation one associated with the mT5 architecture. For more information regarding model tags, refer to the 🤗 [Hub documentation](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined). The output generated from `trainer.push_to_hub()` will be a URL pointing to the Git commit hash, enabling easy access to view the changes made in the model repository!\n",
    "\n",
    "To conclude this section, let's explore fine-tuning mT5 using the low-level features provided by 🤗 Accelerate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning mT5 with 🤗 Accelerate\n",
    "\n",
    "Fine-tuning our model with 🤗 Accelerate mirrors the process we encountered in the text classification example from Chapter 3. However, there are a few distinctions, notably the necessity to explicitly generate summaries during training and define the process for computing the ROUGE scores. Unlike the `Seq2SeqTrainer`, which handled generation during training, we'll have to manage these aspects manually within 🤗 Accelerate. Let's explore how we can incorporate these requirements using 🤗 Accelerate!\n",
    "\n",
    "#### Preparing everything for training\n",
    "To begin, we'll create a DataLoader for each split in our dataset. As PyTorch dataloaders anticipate batches of tensors, we must set the format to \"torch\" within our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our datasets now structured to contain tensors, the subsequent step involves instantiating the `DataCollatorForSeq2Seq` once more. To achieve this, we'll need a fresh version of the model. Let's proceed by reloading it from our cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then instantiate the data collator and use this to define our dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with our setup, the subsequent step involves defining the optimizer we intend to use. Similar to our other examples, we'll opt for AdamW, known for its effectiveness across various problem domains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll provide our model, optimizer, and dataloaders to the `accelerator.prepare()` method to finalize the preparations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our objects are prepared, there are three remaining tasks to complete:\n",
    "\n",
    "1. Define the learning rate schedule.\n",
    "2. Implement a function to post-process the summaries for evaluation.\n",
    "3. Create a repository on the Hub to which we can push our model.\n",
    "\n",
    "For the learning rate schedule, we'll employ the standard linear schedule used in previous sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 10\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To post-process the generated summaries, we require a function that splits them into sentences separated by newlines. This format aligns with the expectation of the ROUGE metric. The following code snippet achieves this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, this structure might ring a bell if you remember how we previously defined the `compute_metrics()` function for the `Seq2SeqTrainer`.\n",
    "\n",
    "Concluding this setup, we're left with creating a model repository on the Hugging Face Hub. We'll utilize the 🤗 Hub library for this task. The process involves defining a name for our repository. The library provides a utility function to combine the repository ID with the user profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"test-bert-finetuned-squad-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this repository name established, we can proceed to clone a local version into our results directory, which will serve as the storage for the training artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "output_dir = \"results-mt5-finetuned-squad-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This action enables us to push the artifacts back to the Hub by invoking the `repo.push_to_hub()` method while training is in progress. Now, let's conclude our analysis by framing the training loop.\n",
    "\n",
    "#### Training loop\n",
    "\n",
    "The training loop for summarization closely resembles the other 🤗 Accelerate examples we've encountered and can be broadly divided into four primary steps:\n",
    "\n",
    "1. Train the model by iterating over all examples in `train_dataloader` for each epoch.\n",
    "2. Generate model summaries at the end of each epoch by initially generating the tokens and then decoding them (along with the reference summaries) into text.\n",
    "3. Compute the ROUGE scores using the methods we previously discussed.\n",
    "4. Save the checkpoints and push everything to the Hub. To expedite this process, we leverage the convenient `blocking=False` argument of the Repository object. This allows us to _asynchronously_ push the checkpoints per epoch, enabling continuous training without waiting for the somewhat sluggish upload of a GB-sized model!\n",
    "\n",
    "These steps can be seen in the following block of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(\n",
    "                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            # Replace -100 in the labels as we can't decode them\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels\n",
    "            )\n",
    "\n",
    "            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    result = rouge_score.compute()\n",
    "    # Extract the median ROUGE scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    print(f\"Epoch {epoch}:\", result)\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a wrap! Running this will yield a model and results akin to those obtained using the Trainer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your fine-tuned model\n",
    "\n",
    "Once you’ve pushed the model to the Hub, you can play with it either via the inference widget or with a pipeline object, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "hub_model_id = \"huggingface-course/mt5-small-finetuned-amazon-en-es\"\n",
    "summarizer = pipeline(\"summarization\", model=hub_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the quality of the summaries, we can provide a few examples from the test set (examples unseen by the model) to our pipeline. Initially, let's create a straightforward function that displays the review, title, and generated summary together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(idx):\n",
    "    review = books_dataset[\"test\"][idx][\"review_body\"]\n",
    "    title = books_dataset[\"test\"][idx][\"review_title\"]\n",
    "    summary = summarizer(books_dataset[\"test\"][idx][\"review_body\"])[0][\"summary_text\"]\n",
    "    print(f\"'>>> Review: {review}'\")\n",
    "    print(f\"\\n'>>> Title: {title}'\")\n",
    "    print(f\"\\n'>>> Summary: {summary}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at one of the English examples we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks promising! The model appears to have executed abstractive summarization by augmenting sections of the review with new words. Moreover, the most fascinating aspect is its bilingual capability, enabling the generation of summaries for Spanish reviews as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That summary translates into \"Very easy to read\" in English, a direct extraction from the review in this instance. Nonetheless, this showcases the versatility of the mT5 model, offering a glimpse into handling a multilingual corpus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5 Stages (each stage has a lab )\n",
    "* duration\n",
    "* Quiz for each stage\n",
    "* Have a notebook for the labs (each stage)\n",
    "* Premeier project and capstone project (Detailed for the student).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
