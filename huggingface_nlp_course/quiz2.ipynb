{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "1. What is the primary function of tokenization within the ðŸ¤— Transformers pipeline?\n",
    "   - A) Loading pre-trained models\n",
    "   - B) Breaking down text into smaller units\n",
    "   - C) Making model inferences\n",
    "   - D) Decoding generated tokens\n",
    "\n",
    "2. Which step involves utilizing a pre-trained transformer model in the pipeline?\n",
    "   - A) Tokenization\n",
    "   - B) Post-processing\n",
    "   - C) Model Loading\n",
    "   - D) Inference\n",
    "\n",
    "3. What is the purpose of the \"Inference\" step in the pipeline?\n",
    "   - A) Converting model outputs\n",
    "   - B) Generating tokenized sequences\n",
    "   - C) Loading text corpora\n",
    "   - D) Using the model to analyze input text\n",
    "\n",
    "4. Which component of the pipeline involves obtaining the final output based on the task performed?\n",
    "   - A) Tokenization\n",
    "   - B) Model Loading\n",
    "   - C) Post-Processing\n",
    "   - D) Inference\n",
    "\n",
    "5. What is the role of post-processing in the ðŸ¤— Transformers pipeline?\n",
    "   - A) Breaking down text into tokens\n",
    "   - B) Loading pre-trained models\n",
    "   - C) Converting model outputs into readable formats\n",
    "   - D) Making inferences on the tokenized input text\n",
    "\n",
    "6. What is the purpose of the `hidden_size` attribute in Transformer models?\n",
    "   - A) Specifies the dimensionality of model input\n",
    "   - B) Defines the number of layers in the model\n",
    "   - C) Specifies the number of sequences processed at once\n",
    "   - D) Specifies the dimension of the hidden states vector\n",
    "   \n",
    "7. Which method is used to load a pretrained Transformer model in the ðŸ¤— Transformers library?\n",
    "   - A) `load_model()`\n",
    "   - B) `from_checkpoint()`\n",
    "   - C) `from_pretrained()`\n",
    "   - D) `init_pretrained_model()`\n",
    "   \n",
    "8. Why is loading a pretrained model often preferred over training a model from scratch for a specific task?\n",
    "   - A) It guarantees better accuracy\n",
    "   - B) It requires less computational resources\n",
    "   - C) It always performs better on new tasks\n",
    "   - D) It eliminates the need for any fine-tuning\n",
    "\n",
    "9. What does the `from_pretrained()` method accomplish in the ðŸ¤— Transformers library?\n",
    "   - A) Downloads and caches the model weights\n",
    "   - B) Initializes the model with random weights\n",
    "   - C) Replaces the model architecture\n",
    "   - D) Deletes the cache folder\n",
    "\n",
    "10. Which attribute holds the number of hidden layers in a Transformer model?\n",
    "   - A) `num_hidden_layers`\n",
    "   - B) `hidden_state_layers`\n",
    "   - C) `layer_hidden_count`\n",
    "   - D) `layer_count`\n",
    "\n",
    "11. Which files are essential components when saving a Transformer model using the ðŸ¤— Transformers library?\n",
    "   - A) `config.json` and `model_state_dict.pt`\n",
    "   - B) `config.json` and `pytorch_model.bin`\n",
    "   - C) `vocab.txt` and `model_weights.pth`\n",
    "   - D) `config.json` and `state_dict.json`\n",
    "\n",
    "12. What purpose does the `pytorch_model.bin` file serve in a saved Transformer model?\n",
    "   - A) Contains metadata about the checkpoint\n",
    "   - B) Stores vocabulary indices for input sequences\n",
    "   - C) Holds all the model's weights\n",
    "   - D) Defines the model's architecture and parameters\n",
    "\n",
    "13. Which function in the ðŸ¤— Transformers library is used to make predictions using a loaded model?\n",
    "   - A) `predict()`\n",
    "   - B) `forward_pass()`\n",
    "   - C) `model.predict()`\n",
    "   - D) `__call__()`\n",
    "\n",
    "14. What role do tokenizers play in preparing text data for processing by Transformer models?\n",
    "   - A) Tokenizers convert text data to numerical IDs only\n",
    "   - B) Tokenizers handle preprocessing tasks like lowercasing and punctuation removal\n",
    "   - C) Tokenizers maintain token-to-ID mappings and structure text for model input\n",
    "   - D) Tokenizers post-process model outputs for human readability\n",
    "\n",
    "15. What does the process of tokenization involve for a Transformer model?\n",
    "   - A) Mapping tokens to word embeddings\n",
    "   - B) Encoding numerical sequences into tokens\n",
    "   - C) Converting text to vocabulary indices or token IDs\n",
    "   - D) Generating tensors from textual inputs\n",
    "\n",
    "16. Which type of tokenization approach involves breaking down raw text into individual words and assigning a numerical representation to each word?\n",
    "   - A) Word-based tokenization\n",
    "   - B) Character-based tokenization\n",
    "   - C) Subword tokenization\n",
    "   - D) Vocabulary-based tokenization\n",
    "\n",
    "17. What issue arises in word-based tokenization due to variations between words with slight differences?\n",
    "   - A) Overuse of the unknown token\n",
    "   - B) Reduced model performance\n",
    "   - C) Ambiguity in token assignments\n",
    "   - D) Different IDs for similar words\n",
    "\n",
    "18. What is a common way to handle out-of-vocabulary words in word-based tokenization?\n",
    "   - A) Assigning a special token for unknown words\n",
    "   - B) Increasing the vocabulary size\n",
    "   - C) Ignoring out-of-vocabulary words\n",
    "   - D) Using subword tokenization for unknown words\n",
    "\n",
    "19. What is a key advantage of character-based tokenizers over word-based tokenizers?\n",
    "   - A) Greater efficiency in processing text\n",
    "   - B) Smaller vocabulary size\n",
    "   - C) Ability to capture semantic meaning of words\n",
    "   - D) Reduced reliance on unknown tokens\n",
    "\n",
    "20. What issue might arise with character-based tokenization concerning spaces and punctuation?\n",
    "   - A) Ambiguity in representing word meanings\n",
    "   - B) Inconsistencies in tokenizing phrases\n",
    "   - C) Difficulty handling languages with complex characters\n",
    "   - D) Challenge in recognizing word boundaries\n",
    "\n",
    "21. What is a primary benefit of using subword tokenization techniques?\n",
    "   - A) Minimizing the need for unknown tokens\n",
    "   - B) Significantly reducing tokenization time\n",
    "   - C) Preserving the original word sequence\n",
    "   - D) Eliminating the need for character-level processing\n",
    "\n",
    "22. What drawback of word-based tokenization does character-based tokenization seek to overcome?\n",
    "   - A) Handling out-of-vocabulary words\n",
    "   - B) Dealing with variations between words\n",
    "   - C) Capturing semantic meanings of characters\n",
    "   - D) Reducing the number of tokens in the vocabulary\n",
    "\n",
    "23. What is the key purpose of subword tokenization algorithms?\n",
    "   - A) Break down words into characters\n",
    "   - B) Generate larger vocabularies\n",
    "   - C) Preserve the meaning of rare words\n",
    "   - D) Ensure word-level representation\n",
    "\n",
    "24. Which tokenization approach can be more effective for languages with extensive vocabularies?\n",
    "   - A) Character-based tokenization\n",
    "   - B) Word-based tokenization\n",
    "   - C) Subword tokenization\n",
    "   - D) Phrase-based tokenization\n",
    "\n",
    "25. What is a challenge associated with character-based tokenization concerning the representation of individual characters?\n",
    "   - A) Limited semantic information per character\n",
    "   - B) Difficulty handling agglutinative languages\n",
    "   - C) Complexity in tokenizing sentences\n",
    "   - D) Large vocabulary size for characters\n",
    "\n",
    "26. What benefit does subword tokenization offer in terms of representing long words?\n",
    "   - A) Reduced reliance on unknown tokens\n",
    "   - B) Maintaining semantic meaning of each character\n",
    "   - C) Consistent word-level representation\n",
    "   - D) Efficient representation using fewer tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
