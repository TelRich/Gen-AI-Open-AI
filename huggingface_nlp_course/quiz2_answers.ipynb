{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions and Answers:\n",
    "\n",
    "1. What is the primary function of tokenization within the ðŸ¤— Transformers pipeline?\n",
    "   - **Answer: B) Breaking down text into smaller units**\n",
    "\n",
    "2. Which step involves utilizing a pre-trained transformer model in the pipeline?\n",
    "   - **Answer: C) Model Loading**\n",
    "\n",
    "3. What is the purpose of the \"Inference\" step in the pipeline?\n",
    "   - **Answer: D) Using the model to analyze input text**\n",
    "\n",
    "4. Which component of the pipeline involves obtaining the final output based on the task performed?\n",
    "   - **Answer: C) Post-Processing**\n",
    "\n",
    "5. What is the role of post-processing in the ðŸ¤— Transformers pipeline?\n",
    "   - **Answer: C) Converting model outputs into readable formats**\n",
    "\n",
    "6. What is the purpose of the `hidden_size` attribute in Transformer models?\n",
    "   - **Answer: D) Specifies the dimension of the hidden states vector**\n",
    "\n",
    "7. Which method is used to load a pretrained Transformer model in the ðŸ¤— Transformers library?\n",
    "   - **Answer: C) `from_pretrained()`**\n",
    "\n",
    "8. Why is loading a pretrained model often preferred over training a model from scratch for a specific task?\n",
    "   - **Answer: B) It requires less computational resources**\n",
    "\n",
    "9. What does the `from_pretrained()` method accomplish in the ðŸ¤— Transformers library?\n",
    "   - **Answer: A) Downloads and caches the model weights**\n",
    "\n",
    "10. Which attribute holds the number of hidden layers in a Transformer model?\n",
    "   - **Answer: A) `num_hidden_layers`**\n",
    "\n",
    "11. Which files are essential components when saving a Transformer model using the ðŸ¤— Transformers library?\n",
    "   - **Answer: B) `config.json` and `pytorch_model.bin`**\n",
    "\n",
    "12. What purpose does the `pytorch_model.bin` file serve in a saved Transformer model?\n",
    "   - **Answer: C) Holds all the model's weights**\n",
    "\n",
    "13. Which function in the ðŸ¤— Transformers library is used to make predictions using a loaded model?\n",
    "   - **Answer: C) `model.predict()`**\n",
    "\n",
    "14. What role do tokenizers play in preparing text data for processing by Transformer models?\n",
    "   - **Answer: C) Tokenizers maintain token-to-ID mappings and structure text for model input**\n",
    "\n",
    "15. What does the process of tokenization involve for a Transformer model?\n",
    "   - **Answer: C) Converting text to vocabulary indices or token IDs**\n",
    "\n",
    "16. Which type of tokenization approach involves breaking down raw text into individual words and assigning a numerical representation to each word?\n",
    "   - **Answer: A) Word-based tokenization**\n",
    "\n",
    "17. What issue arises in word-based tokenization due to variations between words with slight differences?\n",
    "   - **Answer: D) Different IDs for similar words**\n",
    "\n",
    "18. What is a common way to handle out-of-vocabulary words in word-based tokenization?\n",
    "   - **Answer: A) Assigning a special token for unknown words**\n",
    "\n",
    "19. What is a key advantage of character-based tokenizers over word-based tokenizers?\n",
    "   - **Answer:** **B) Smaller vocabulary size**\n",
    "\n",
    "20. What issue might arise with character-based tokenization concerning spaces and punctuation?\n",
    "   - **Answer:** **D) Challenge in recognizing word boundaries**\n",
    "\n",
    "21. What is a primary benefit of using subword tokenization techniques?\n",
    "   - **Answer:** **A) Minimizing the need for unknown tokens**\n",
    "\n",
    "22. What drawback of word-based tokenization does character-based tokenization seek to overcome?\n",
    "   - **Answer:** **B) Dealing with variations between words**\n",
    "\n",
    "23. What is the key purpose of subword tokenization algorithms?\n",
    "   - **Answer:** **C) Preserve the meaning of rare words**\n",
    "\n",
    "24. Which tokenization approach can be more effective for languages with extensive vocabularies?\n",
    "   - **Answer:** **A) Character-based tokenization**\n",
    "\n",
    "25. What is a challenge associated with character-based tokenization concerning the representation of individual characters?\n",
    "   - **Answer:** **A) Limited semantic information per character**\n",
    "\n",
    "26. What benefit does subword tokenization offer in terms of representing long words?\n",
    "   - **Answer:** **D) Efficient representation using fewer tokens**\n",
    "\n",
    "27. What is the purpose of the `decode()` method in tokenization?\n",
    "   - **Answer**: **B) Reverse tokenization into a human-readable string**\n",
    "\n",
    "28. What does padding accomplish in the context of handling multiple sequences?\n",
    "   - **Answer**: **C) Helps create a tensor with consistent dimensions**\n",
    "\n",
    "29. Why is an attention mask crucial in tokenizing sequences with padding?\n",
    "   - **Answer**: **A) It instructs the model to ignore padding tokens during processing**\n",
    "\n",
    "30. What is a common solution for handling longer sequences when the model has a sequence length limit?\n",
    "   - **Answer**: **C) Using a model with a longer supported sequence length**\n",
    "\n",
    "31. What purpose do special tokens like `[CLS]` and `[SEP]` serve in tokenization?\n",
    "   - **Answer**: **A) Represent the start and end of sequences**\n",
    "\n",
    "32. Which method within the tokenizer object provides inputs ready to be fed into a model?\n",
    "   - **Answer**: **D) Direct invocation of the tokenizer object on a sentence**\n",
    "\n",
    "33. What is the primary advantage of the `decode()` method when applied to sequences?\n",
    "   - **Answer**: **C) Produces a human-readable string from numerical representations**\n",
    "\n",
    "34. How does padding support the processing of multiple sequences within a batch?\n",
    "   - **Answer**: **C) It adds zeros to shorter sequences to ensure uniform lengths**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
