{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2 Notebook\n",
    "\n",
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. \n",
    "Preprocessing with a tokenizer. \n",
    "\n",
    "You're to preprocess and display the output of the below sentence for the model. The tokenizer has already been loaded for you.\n",
    "\n",
    "* \"The universe is vast and full of mysteries.\",\n",
    "* \"Exploring the depths of the ocean reveals hidden wonders.\"\n",
    "\n",
    "Set the following parameters:\n",
    "\n",
    "- `padding=True`: This parameter ensures that the sequences are padded to equal lengths if their lengths vary.\n",
    "- `truncation=True`: If any sentence exceeds the maximum length that the model can handle, truncation cuts it to fit within that limit.\n",
    "- `return_tensors=\"pt\"`: This specifies that the tokenizer should return PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996,  5304,  2003,  6565,  1998,  2440,  1997, 15572,  1012,\n",
      "           102,     0],\n",
      "        [  101, 11131,  1996, 11143,  1997,  1996,  4153,  7657,  5023, 16278,\n",
      "          1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "\n",
    "# Different set of raw inputs\n",
    "different_raw_inputs = [\n",
    "    \"The universe is vast and full of mysteries.\",\n",
    "    \"Exploring the depths of the ocean reveals hidden wonders.\",\n",
    "]\n",
    "\n",
    "# Tokenize the new set of inputs\n",
    "inputs = tokenizer(different_raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. \n",
    "Going through the model\n",
    "\n",
    "Feed the preprocessed text into the model and display the vector output of the transformer. The model has already been loaded for you.\n",
    "\n",
    "What are the batch size, sequence length and hidden size from the vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch size is 2\n",
      "The sequence lenght is 12\n",
      "The hidden size is 768\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "\n",
    "outputs = model(**inputs)\n",
    "shape = outputs.last_hidden_state.shape\n",
    "print(f\"The batch size is {shape[0]}\")\n",
    "print(f\"The sequence lenght is {shape[1]}\")\n",
    "print(f\"The hidden size is {shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. \n",
    "Making sense out of the numbers\n",
    "\n",
    "Instead of just extracting features from the input text, we want to actually categorize it as positive or negative. To do this, we need a specialized model head called a \"sequence classification head\" that takes the extracted features and predicts a category for the sentence.\n",
    "\n",
    "While the `AutoModel` class can extract features, it's not designed for classification. Therefore, we'll use `AutoModelForSequenceClassification` instead, which comes with this specialized head built-in.\n",
    "\n",
    "The model has been loaded for you, Pass in the preprocess text and display the logits and it's shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[-3.8570,  4.1280],\n",
      "        [-4.2169,  4.5045]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "The output from the model are not probabilities but logits which are unnormalized scores outputted by the last layer of the models. \n",
    "The softmax function is used to convert these logits into probabilities. It takes an array of numbers (logits) as input and returns. Convert those output to probabilities and display them, also display their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.4041e-04, 9.9966e-01],\n",
      "        [1.6303e-04, 9.9984e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "print(predictions)\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. \n",
    "#### Write your conclusions from the model prediction below:\n",
    "\n",
    "Now we can conclude that the model predicted the following:\n",
    "\n",
    "First sentence: NEGATIVE: 0.00034041, POSITIVE: 0.9966\n",
    "Second sentence: NEGATIVE: 0.00016303, POSITIVE: 0.99984"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    " Replicate Tokenization and Conversion to Input IDs\n",
    "\n",
    "Replicate the tokenization and conversion to input IDs for the following input sentences:\n",
    "1. \"HuggingFace provides amazing NLP resources.\",\n",
    "2. \"I love learning about natural language processing!\",\n",
    "\n",
    "\n",
    "Using the appropriate tokenizer, tokenize these sentences and convert them into input IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hu', '##gging', '##F', '##ace', 'provides', 'amazing', 'NL', '##P', 'resources', '.', 'I', 'love', 'learning', 'about', 'natural', 'language', 'processing', '!']\n",
      "[20164, 10932, 2271, 7954, 2790, 6929, 21239, 2101, 3979, 119, 146, 1567, 3776, 1164, 2379, 1846, 6165, 106]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = [\n",
    "    \"HuggingFace provides amazing NLP resources.\",\n",
    "    \"I love learning about natural language processing!\",\n",
    "]\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "Decoding\n",
    "\n",
    "Convert the vocabulary indices gotten from above back into a human-readable text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace provides amazing NLP resources. I love learning about natural language processing!\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "\n",
    "decoded_string = tokenizer.decode([20164, 10932, 2271, 7954, 2790, 6929, 21239, 2101, 3979, 119, 146, 1567, 3776, 1164, 2379, 1846, 6165, 106])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "\n",
    "What are the tokenized representations of the following sequences: \n",
    "1. 'The mountains are calling and I must go.'\n",
    "2. 'Nature always wears the colors of the spirit.' \n",
    "These sequences are tokenized using the 'distilbert-base-uncased-finetuned-sst-2-english' checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'mountains', 'are', 'calling', 'and', 'i', 'must', 'go', '.', 'nature', 'always', 'wears', 'the', 'colors', 'of', 'the', 'spirit', '.']\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "sequence = [\n",
    "    'The mountains are calling and I must go.',\n",
    "    'Nature always wears the colors of the spirit.' \n",
    "]\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "\n",
    "What are the different tokenization and padding methods applied to the sequences: \n",
    "\n",
    "1. 'The road to success is always under construction.'\n",
    "2. 'A journey of a thousand miles begins with a single step.'\n",
    "\n",
    "These sequences are tokenized using the 'distilbert-base-uncased-finetuned-sst-2-english' checkpoint. The methods employed include padding to the longest sequence, padding to the model's maximum sequence length, padding to a specified maximum length (10), truncation to the model's maximum sequence length, truncation to a specified maximum length (10), and returning the tokenized sequences as PyTorch tensors and NumPy arrays with padding enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding to longest sequence:\n",
      "{'input_ids': [[101, 1996, 2346, 2000, 3112, 2003, 2467, 2104, 2810, 1012, 102, 0, 0, 0], [101, 1037, 4990, 1997, 1037, 4595, 2661, 4269, 2007, 1037, 2309, 3357, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "Padding to model's max length:\n",
      "{'input_ids': [[101, 1996, 2346, 2000, 3112, 2003, 2467, 2104, 2810, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 4990, 1997, 1037, 4595, 2661, 4269, 2007, 1037, 2309, 3357, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "\n",
      "Padding to specified max length (10):\n",
      "{'input_ids': [[101, 1996, 2346, 2000, 3112, 2003, 2467, 2104, 2810, 1012, 102], [101, 1037, 4990, 1997, 1037, 4595, 2661, 4269, 2007, 1037, 2309, 3357, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "Truncation to model's max length:\n",
      "{'input_ids': [[101, 1996, 2346, 2000, 3112, 2003, 2467, 2104, 2810, 1012, 102], [101, 1037, 4990, 1997, 1037, 4595, 2661, 4269, 2007, 1037, 2309, 3357, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "Truncation to specified max length (10):\n",
      "{'input_ids': [[101, 1996, 2346, 2000, 3112, 2003, 2467, 2104, 2810, 102], [101, 1037, 4990, 1997, 1037, 4595, 2661, 4269, 2007, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "Returning PyTorch tensors:\n",
      "{'input_ids': tensor([[ 101, 1996, 2346, 2000, 3112, 2003, 2467, 2104, 2810, 1012,  102,    0,\n",
      "            0,    0],\n",
      "        [ 101, 1037, 4990, 1997, 1037, 4595, 2661, 4269, 2007, 1037, 2309, 3357,\n",
      "         1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Returning NumPy arrays:\n",
      "{'input_ids': array([[ 101, 1996, 2346, 2000, 3112, 2003, 2467, 2104, 2810, 1012,  102,\n",
      "           0,    0,    0],\n",
      "       [ 101, 1037, 4990, 1997, 1037, 4595, 2661, 4269, 2007, 1037, 2309,\n",
      "        3357, 1012,  102]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Different context for the sequences\n",
    "different_sequences = [\n",
    "    \"The road to success is always under construction.\",\n",
    "    \"A journey of a thousand miles begins with a single step.\"\n",
    "]\n",
    "\n",
    "# Padding to the longest sequence in the new context\n",
    "model_inputs = tokenizer(different_sequences, padding=\"longest\")\n",
    "print(\"Padding to longest sequence:\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Padding to the model's max sequence length\n",
    "model_inputs = tokenizer(different_sequences, padding=\"max_length\")\n",
    "print(\"\\nPadding to model's max length:\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Padding to a specified max length\n",
    "model_inputs = tokenizer(different_sequences, padding=\"max_length\", max_length=10)\n",
    "print(\"\\nPadding to specified max length (10):\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Truncation to the model's max sequence length\n",
    "model_inputs = tokenizer(different_sequences, truncation=True)\n",
    "print(\"\\nTruncation to model's max length:\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Truncation to a specified max length\n",
    "model_inputs = tokenizer(different_sequences, max_length=10, truncation=True)\n",
    "print(\"\\nTruncation to specified max length (10):\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Tokenizing sequences and returning PyTorch tensors\n",
    "model_inputs = tokenizer(different_sequences, padding=True, return_tensors=\"pt\")\n",
    "print(\"\\nReturning PyTorch tensors:\")\n",
    "print(model_inputs)\n",
    "\n",
    "# Tokenizing sequences and returning NumPy arrays\n",
    "model_inputs = tokenizer(different_sequences, padding=True, return_tensors=\"np\")\n",
    "print(\"\\nReturning NumPy arrays:\")\n",
    "print(model_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
