{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "1. Which of the following is **not** an application of NLP?\n",
    "   - A) Sentiment Analysis\n",
    "   - B) Image Recognition\n",
    "   - C) Text Classification\n",
    "   - D) Named Entity Recognition\n",
    "\n",
    "2. Which NLP task involves converting spoken language into text?\n",
    "   - A) Sentiment Analysis\n",
    "   - B) Named Entity Recognition\n",
    "   - C) Speech Recognition\n",
    "   - D) Text Generation\n",
    "\n",
    "3. What is the primary goal of NLP?\n",
    "   - A) Creating human-like chatbots\n",
    "   - B) Analyzing numerical data\n",
    "   - C) Enabling computers to understand human language\n",
    "   - D) Generating images from text\n",
    "\n",
    "4. Which framework or library is commonly used for NLP tasks?\n",
    "   - A) TensorFlow\n",
    "   - B) Matplotlib\n",
    "   - C) pandas\n",
    "   - D) NLTK\n",
    "\n",
    "5. Which type of system uses NLP to answer questions based on a given text or knowledge base?\n",
    "   - A) Chatbots\n",
    "   - B) Machine Translation systems\n",
    "   - C) Sentiment Analysis systems\n",
    "   - D) Speech Recognition systems\n",
    "\n",
    "6. Which of the following best describes the challenge of resolving references like pronouns in a sentence?\n",
    "   - A) Negation resolution\n",
    "   - B) Anaphora resolution\n",
    "   - C) Syntax and Semantics parsing\n",
    "   - D) Coreference resolution\n",
    "\n",
    "7. What makes understanding the meaning of a sentence challenging in NLP?\n",
    "   - A) Lack of standardization\n",
    "   - B) Multimodal integration\n",
    "   - C) Sarcasm and Irony recognition\n",
    "   - D) Ambiguity and context dependency\n",
    "\n",
    "8. Which factor makes training effective NLP models challenging due to the scarcity of high-quality data?\n",
    "   - A) Data sparsity\n",
    "   - B) Variability in language\n",
    "   - C) Bias and fairness\n",
    "   - D) Privacy and security concerns\n",
    "\n",
    "9. Handling sarcasm, irony, and humor in text often relies on:\n",
    "   - A) Negation and double negation\n",
    "   - B) Syntax and semantics parsing\n",
    "   - C) Context, tone, and cultural knowledge\n",
    "   - D) Anaphora resolution\n",
    "\n",
    "10. What poses a significant challenge in ensuring the fairness and mitigation of biases in NLP models?\n",
    "   - A) Continuous evolution of language\n",
    "   - B) Multimodal challenges\n",
    "   - C) Lack of standardization\n",
    "   - D) Inheritance and amplification of biases from training data\n",
    "\n",
    "11. Which task can Transformers perform effectively by generating concise summaries of lengthy text documents?\n",
    "   - A) Image Captioning\n",
    "   - B) Named Entity Recognition (NER)\n",
    "   - C) Text Summarization\n",
    "   - D) Sentiment Analysis\n",
    "\n",
    "12. What role do Transformers play in assisting systems like Siri, Alexa, and chatbots?\n",
    "   - A) Image Captioning\n",
    "   - B) Sentiment Analysis\n",
    "   - C) Language Translation\n",
    "   - D) Language Understanding and Generation in Conversations\n",
    "\n",
    "13. Transformers contribute significantly to machine translation systems like Google Translate by:\n",
    "   - A) Generating human-like text\n",
    "   - B) Understanding user input in real-time\n",
    "   - C) Enabling the translation of text across languages\n",
    "   - D) Identifying and classifying named entities in text\n",
    "\n",
    "14. Which task involves Transformers collaborating with computer vision models to generate textual descriptions for images?\n",
    "   - A) Language Understanding\n",
    "   - B) Question Answering\n",
    "   - C) Image Captioning\n",
    "   - D) Text Generation\n",
    "\n",
    "15. Transformers excel in which task, identifying and categorizing entities such as people, organizations, and locations in text?\n",
    "   - A) Text Classification\n",
    "   - B) Named Entity Recognition (NER)\n",
    "   - C) Text Summarization\n",
    "   - D) Document Classification\n",
    "\n",
    "16. What is the primary purpose of using pipelines in NLP?\n",
    "   - A) To define the sequence of NLP tasks\n",
    "   - B) To manually process text data\n",
    "   - C) To evaluate NLP libraries\n",
    "   - D) To perform post-processing on text data\n",
    "\n",
    "17. Which step involves configuring a sequence of NLP tasks such as tokenization, named entity recognition, and sentiment analysis?\n",
    "   - A) Loading or Preprocessing Data\n",
    "   - B) Defining the Pipeline\n",
    "   - C) Accessing Results\n",
    "   - D) Customization\n",
    "\n",
    "18. After defining the pipeline, what does instantiating the pipeline involve?\n",
    "   - A) Applying the pipeline to text data\n",
    "   - B) Reading text from files\n",
    "   - C) Configuring the sequence of NLP operations\n",
    "   - D) Adding custom components for domain-specific tasks\n",
    "\n",
    "19. Which step involves running the predefined NLP tasks on text data, handling intermediate results?\n",
    "   - A) Customization\n",
    "   - B) Accessing Results\n",
    "   - C) Process Data\n",
    "   - D) Define the Pipeline\n",
    "\n",
    "20. What does post-processing in the context of working with NLP pipelines entail?\n",
    "   - A) Instantiating the pipeline\n",
    "   - B) Customizing the pipeline components\n",
    "   - C) Aggregating information or integrating results\n",
    "   - D) Loading or Preprocessing Data\n",
    "\n",
    "21. What component of the Transformer architecture addresses the lack of inherent token order in sequences?\n",
    "   - A) Input Embedding\n",
    "   - B) Positional Encoding\n",
    "   - C) Self-Attention Mechanism\n",
    "   - D) Position-wise Feedforward Networks\n",
    "\n",
    "22. Which mechanism allows each token to consider relationships and dependencies between all other tokens in a sequence?\n",
    "   - A) Self-Attention Mechanism\n",
    "   - B) Masking\n",
    "   - C) Encoder-Decoder Architecture\n",
    "   - D) Position-wise Feedforward Networks\n",
    "\n",
    "23. In Transformer architecture, what is responsible for capturing the semantic meaning of tokens in a sequence?\n",
    "   - A) Positional Encoding\n",
    "   - B) Self-Attention Mechanism\n",
    "   - C) Input Embedding\n",
    "   - D) Residual Connections\n",
    "\n",
    "24. What purpose do residual connections and layer normalization serve in Transformers?\n",
    "   - A) Enhancing model's capacity for complex relationships\n",
    "   - B) Facilitating training of deeper networks and improving gradient flow\n",
    "   - C) Generating output sequences step by step\n",
    "   - D) Capturing token order in sequences\n",
    "\n",
    "25. What is the primary function of the final layer in the Transformer architecture?\n",
    "   - A) Generating positional encodings\n",
    "   - B) Applying self-attention mechanisms\n",
    "   - C) Producing the model's predictions\n",
    "   - D) Handling masking for variable-length sequences\n",
    "\n",
    "26. Which mechanism allows encoder models to capture dependencies and context effectively by enabling each token to attend to all other tokens in the sequence?\n",
    "   - A) Layer Stacking\n",
    "   - B) Residual Connections\n",
    "   - C) Self-Attention Mechanism\n",
    "   - D) Dimension Reduction\n",
    "\n",
    "27. What is the primary focus of encoder models in the context of natural language processing (NLP) tasks?\n",
    "   - A) Generating sequential output\n",
    "   - B) Decoding input sequences\n",
    "   - C) Encoding and understanding input data\n",
    "   - D) Performing autoregressive generation\n",
    "\n",
    "28. Which component of the encoder model is responsible for transforming each token into a fixed-dimensional vector representation?\n",
    "   - A) Layer Stacking\n",
    "   - B) Positional Encoding\n",
    "   - C) Self-Attention Mechanism\n",
    "   - D) Input Embedding\n",
    "\n",
    "29. How do encoder models typically update token embeddings to produce contextual representations?\n",
    "   - A) Through sequential output generation\n",
    "   - B) Using autoregressive generation\n",
    "   - C) By capturing token relationships with other tokens\n",
    "   - D) Dimension reduction in hidden layers\n",
    "\n",
    "30. What is a notable characteristic of encoder models that contributes to their adaptability for various NLP tasks?\n",
    "   - A) Handling multimodal input\n",
    "   - B) Autoregressive generation mechanism\n",
    "   - C) Contextual representations from input sequences\n",
    "   - D) Layer stacking for sequence generation\n",
    "\n",
    "31. Which mechanism in decoder models allows them to focus on different parts of the input context or previously generated tokens when generating the current token?\n",
    "   - A) Layer Stacking\n",
    "   - B) Residual Connections\n",
    "   - C) Attention Mechanisms\n",
    "   - D) Dimension Reduction\n",
    "\n",
    "32. In the context of natural language processing (NLP) tasks, what is the primary task of a decoder model?\n",
    "   - A) Encoding and understanding input data\n",
    "   - B) Generating sequential output\n",
    "   - C) Performing autoregressive generation\n",
    "   - D) Dimension reduction in hidden layers\n",
    "\n",
    "33. What role does the context or \"thought vector\" play in the operation of a decoder model?\n",
    "   - A) Generating token embeddings\n",
    "   - B) Sequential output generation\n",
    "   - C) Encoding input sequences\n",
    "   - D) Providing relevant information from input data\n",
    "\n",
    "34. Which feature of decoder models allows them to evolve the hidden state or context with each generated token in autoregressive generation?\n",
    "   - A) Layer Stacking\n",
    "   - B) Attention Mechanisms\n",
    "   - C) Residual Connections\n",
    "   - D) Dimension Reduction\n",
    "\n",
    "35. What is a notable characteristic of decoder models that contributes to their versatility in generating structured sequences?\n",
    "   - A) Conditional generation\n",
    "   - B) Handling multimodal input\n",
    "   - C) Contextual representations from input sequences\n",
    "   - D) Layer stacking for sequence generation\n",
    "\n",
    "36. What is one of the potential issues caused by training transformer models on large datasets?\n",
    "   - A) Out-of-distribution data issues\n",
    "   - B) Contextual bias\n",
    "   - C) Data bias, including perpetuating existing biases\n",
    "   - D) Lack of interpretability\n",
    "\n",
    "37. Which challenge arises when transformer models encounter significantly different data from what they were trained on?\n",
    "   - A) Computational resource constraints\n",
    "   - B) Long sequence limitations\n",
    "   - C) Misinformation and factually incorrect outputs\n",
    "   - D) Struggle with out-of-distribution data\n",
    "\n",
    "38. What is a strategy to address bias in transformer models during training?\n",
    "   - A) Multilingual focus\n",
    "   - B) Lack of explanation in outputs\n",
    "   - C) Fairness metrics and debiasing training data\n",
    "   - D) Handling contextual bias\n",
    "\n",
    "39. What potential issue can arise due to the training data containing inaccuracies in transformer models?\n",
    "   - A) Generating offensive content\n",
    "   - B) Computational resource constraints\n",
    "   - C) Lack of interpretability\n",
    "   - D) Misinformation and misleading outputs\n",
    "\n",
    "40. Which factor contributes to the computational intensity of training and deploying transformer models?\n",
    "   - A) Long sequence limitations\n",
    "   - B) Fairness and bias mitigation\n",
    "   - C) Monolingual focus\n",
    "   - D) Requirement of substantial computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the scores (in 4 decimal places) for the labels in the code below?\n",
    "\n",
    "```python\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Text to classify\n",
    "text_to_classify = \"The latest scientific research suggests a breakthrough in renewable energy sources.\"\n",
    "\n",
    "# Candidate labels/categories\n",
    "candidate_labels = [\"technology\", \"environment\", \"health\"]\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(text_to_classify, candidate_labels)\n",
    "\n",
    "print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why might the following code not work when using the fill-mask pipeline from the Transformers library?\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the fill-mask pipeline\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "\n",
    "# Text with a mask to fill\n",
    "text_with_mask = \"Exploring the depths of is an exciting adventure.\"\n",
    "\n",
    "# Perform the masked language modeling\n",
    "filled_mask = unmasker(text_with_mask, top_k=3)\n",
    "```\n",
    "\n",
    "Choose the correct reason why the code might not work:\n",
    "\n",
    "* A) The text provided for filling the mask does not contain a valid mask token (\"\\<mask\\>\").\n",
    "* B) The \"fill-mask\" pipeline requires complete sentences, and the text is incomplete.\n",
    "* C) The top_k parameter should be set to a numerical value rather than a list of options.\n",
    "* D) The pipeline is not imported correctly, causing an error in the code execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the result of the below code?\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the Named Entity Recognition (NER) pipeline\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "# Text for named entity recognition\n",
    "new_text_to_analyze = \"During the space mission, Neil Armstrong landed on the moon in 1969.\"\n",
    "\n",
    "# Perform Named Entity Recognition (NER)\n",
    "new_ner_result = ner(new_text_to_analyze)\n",
    "\n",
    "print(new_ner_result)\n",
    "```\n",
    "Choose the correct reult for this code:\n",
    "* A) {'entity_group': 'LOC', 'score': 0.99813056, 'word': 'Buzz Aldrin', 'start': 26, 'end': 40}\n",
    "* B) {'entity_group': 'ORG', 'score': 0.997652', 'word': 'Neil Armstrong', 'start': 20, 'end': 44}\n",
    "* C) {'entity_group': 'PER', 'score': 0.99813056, 'word': 'Neil Armstrong', 'start': 26, 'end': 40}\n",
    "* D) {'entity_group': 'DATE', 'score': 0.99813056, 'word': 'Armstrong Neil', 'start': 28, 'end': 38}\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
