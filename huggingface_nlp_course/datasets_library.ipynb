{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Library**\n",
    "\n",
    "In this chapter, you'll delve deeper into the capabilities of the ðŸ¤— Datasets library. Here are some of the key questions you'll explore:\n",
    "\n",
    "1. How to handle datasets not available on the Hugging Face Hub?\n",
    "2. Techniques for slicing, dicing, and working with datasets, including using Pandas.\n",
    "3. Handling large datasets that might overwhelm your system's RAM.\n",
    "4. Understanding concepts like memory mapping and Apache Arrow.\n",
    "5. Creating custom datasets and contributing them to the Hugging Face Hub.\n",
    "\n",
    "Let's embark on this journey to enhance your understanding of ðŸ¤— Datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What should I do if my dataset isn't available on the Hugging Face Hub?\n",
    "\n",
    "You've learned how to utilize the Hugging Face Hub to fetch datasets, but there will be instances where you need to work with data stored locally on your laptop or on a remote server. In this section, we'll explore how ðŸ¤— Datasets can be employed to load datasets that aren't accessible on the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with local and remote datasets\n",
    "\n",
    "ðŸ¤— Datasets simplifies the loading of local and remote datasets by providing loading scripts for various common data formats. Here are examples of loading scripts for different data formats:\n",
    "\n",
    "- CSV & TSV: `load_dataset(\"csv\", data_files=\"my_file.csv\")`\n",
    "- Text files: `load_dataset(\"text\", data_files=\"my_file.txt\")`\n",
    "- JSON & JSON Lines: `load_dataset(\"json\", data_files=\"my_file.jsonl\")`\n",
    "- Pickled DataFrames: `load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")`\n",
    "\n",
    "The above illustrates that for each data format, specifying the type of loading script in the `load_dataset()` function is sufficient. Additionally, the `data_files` argument is used to provide the path to one or more files. Let's begin by loading a dataset from local files, and subsequently, we'll explore how to achieve the same with remote files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a local dataset\n",
    "\n",
    "For this example weâ€™ll use the SQuAD-it dataset, which is a large-scale dataset for question answering in Italian.\n",
    "\n",
    "The training and test splits are hosted on GitHub, so we can download them using the blow link:\n",
    "\n",
    "https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "\n",
    "https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
    "\n",
    "Once you've downloaded them, unzip the files. You can see the compressed files has SQuAD_it-train.json and SQuAD_it-test.json, and that the data is stored in the JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a JSON file using the `load_dataset()` function involves specifying whether the dataset is in standard JSON format (resembling a nested dictionary) or JSON Lines format (JSON separated by lines). In datasets like SQuAD-it, the information is stored in a nested structure, often with text contained within a specific field. To load this dataset correctly, we'd specify the `field` argument to accommodate this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading datasets from local files, the default behavior is to generate a DatasetDict object containing at least one split, typically the train split. To verify this, you can inspect the `squad_it_dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output displays the count of rows along with the column names present in the training set. You can explore individual examples by selecting one from the train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_it_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the right approach! Having both the train and test splits within a single DatasetDict object allows for more efficient handling. Mapping each split name to its respective file using the data_files argument ensures the inclusion of both splits in a unified dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having both splits within a unified object facilitates uniform preprocessing across the entire dataset, ensuring consistency in the applied transformations or cleaning methods.\n",
    "\n",
    "Datasets simplifies the process by handling file decompression automatically. Using compressed files directly in the `data_files` argument streamlines the loading process without the need for pre-decompression steps. This means we could have skipped the process of unzipping hte file manually by pointing the `data_files` argument directly to the compressed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether it's ZIP, TAR, or other common compression formats, ðŸ¤— Datasets conveniently handles the decompression process upon loading, ensuring ease of use when working with compressed files directly in the `data_files` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a remote dataset\n",
    "\n",
    "When handling remote datasets, the process remains straightforward. Instead of directing the `data_files` argument to local paths, you simply assign it the URLs where the remote files are located. For instance, in the case of the SQuAD-it dataset residing on GitHub, the `data_files` parameter can point directly to the URLs hosting the SQuAD_it-*.json.gz files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This retrieves the identical DatasetDict object we previously obtained, eliminating the need for manual downloading and decompression of the SQuAD_it-*.json.gz files. With this dataset at hand, let's delve into diverse data manipulation techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toying with data subsets\n",
    "\n",
    "In this section, we'll explore various techniques for slicing and dicing data using the ðŸ¤— Datasets library. We'll cover operations like selecting specific columns, filtering rows based on conditions, and shuffling the dataset\n",
    "\n",
    "Beyond Dataset.map(), ðŸ¤— Datasets offers a range of methods to manage datasets. These functions empower you to filter rows, select columns, shuffle data, and more. Let's explore some of these to enhance our dataset manipulations.\n",
    "\n",
    "In this instance, we'll work with the Drug Review Dataset available on the UC Irvine Machine Learning Repository. It includes patient reviews concerning different drugs, along with the treated condition and a 10-star rating reflecting patient satisfaction.\n",
    "\n",
    "First we need to download and extract the data\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "100 94133    0 94133    0     0  22839      0 --:--:--  0:00:04 --:--:-- 22847\n",
      "100  787k    0  787k    0     0   156k      0 --:--:--  0:00:05 --:--:--  156k\n",
      "100 1823k    0 1823k    0     0   304k      0 --:--:--  0:00:05 --:--:--  304k\n",
      "100 2875k    0 2875k    0     0   410k      0 --:--:--  0:00:07 --:--:--  525k\n",
      "100 3807k    0 3807k    0     0   475k      0 --:--:--  0:00:08 --:--:--  764k\n",
      "100 4747k    0 4747k    0     0   527k      0 --:--:--  0:00:08 --:--:--  954k\n",
      "100 5455k    0 5455k    0     0   545k      0 --:--:--  0:00:09 --:--:--  938k\n",
      "100 6223k    0 6223k    0     0   565k      0 --:--:--  0:00:10 --:--:--  880k\n",
      "100 7087k    0 7087k    0     0   590k      0 --:--:--  0:00:11 --:--:--  844k\n",
      "100 8195k    0 8195k    0     0   629k      0 --:--:--  0:00:13 --:--:--  875k\n",
      "100 8959k    0 8959k    0     0   640k      0 --:--:--  0:00:13 --:--:--  842k\n",
      "100 9943k    0 9943k    0     0   662k      0 --:--:--  0:00:15 --:--:--  893k\n",
      "100 10.6M    0 10.6M    0     0   681k      0 --:--:--  0:00:15 --:--:--  934k\n",
      "100 11.6M    0 11.6M    0     0   701k      0 --:--:--  0:00:16 --:--:--  968k\n",
      "100 12.5M    0 12.5M    0     0   716k      0 --:--:--  0:00:17 --:--:--  942k\n",
      "100 14.0M    0 14.0M    0     0   756k      0 --:--:--  0:00:18 --:--:-- 1082k\n",
      "100 14.5M    0 14.5M    0     0   745k      0 --:--:--  0:00:19 --:--:--  997k\n",
      "100 15.3M    0 15.3M    0     0   749k      0 --:--:--  0:00:21 --:--:--  968k\n",
      "100 16.2M    0 16.2M    0     0   754k      0 --:--:--  0:00:21 --:--:--  932k\n",
      "100 16.9M    0 16.9M    0     0   753k      0 --:--:--  0:00:22 --:--:--  884k\n",
      "100 17.5M    0 17.5M    0     0   750k      0 --:--:--  0:00:24 --:--:--  724k\n",
      "100 18.2M    0 18.2M    0     0   748k      0 --:--:--  0:00:24 --:--:--  762k\n",
      "100 18.9M    0 18.9M    0     0   747k      0 --:--:--  0:00:25 --:--:--  737k\n",
      "100 19.8M    0 19.8M    0     0   751k      0 --:--:--  0:00:27 --:--:--  742k\n",
      "100 20.5M    0 20.5M    0     0   749k      0 --:--:--  0:00:27 --:--:--  735k\n",
      "100 21.1M    0 21.1M    0     0   747k      0 --:--:--  0:00:28 --:--:--  737k\n",
      "100 21.8M    0 21.8M    0     0   744k      0 --:--:--  0:00:29 --:--:--  721k\n",
      "100 22.4M    0 22.4M    0     0   741k      0 --:--:--  0:00:30 --:--:--  708k\n",
      "100 23.3M    0 23.3M    0     0   745k      0 --:--:--  0:00:31 --:--:--  711k\n",
      "100 24.2M    0 24.2M    0     0   752k      0 --:--:--  0:00:33 --:--:--  769k\n",
      "100 25.1M    0 25.1M    0     0   756k      0 --:--:--  0:00:33 --:--:--  804k\n",
      "100 25.8M    0 25.8M    0     0   757k      0 --:--:--  0:00:34 --:--:--  835k\n",
      "100 26.6M    0 26.6M    0     0   759k      0 --:--:--  0:00:35 --:--:--  871k\n",
      "100 27.3M    0 27.3M    0     0   757k      0 --:--:--  0:00:36 --:--:--  833k\n",
      "100 28.0M    0 28.0M    0     0   755k      0 --:--:--  0:00:37 --:--:--  770k\n",
      "100 28.9M    0 28.9M    0     0   758k      0 --:--:--  0:00:39 --:--:--  775k\n",
      "100 29.9M    0 29.9M    0     0   766k      0 --:--:--  0:00:40 --:--:--  830k\n",
      "100 31.0M    0 31.0M    0     0   774k      0 --:--:--  0:00:40 --:--:--  883k\n",
      "100 32.0M    0 32.0M    0     0   781k      0 --:--:--  0:00:42 --:--:--  956k\n",
      "100 33.1M    0 33.1M    0     0   790k      0 --:--:--  0:00:42 --:--:-- 1054k\n",
      "100 34.2M    0 34.2M    0     0   797k      0 --:--:--  0:00:43 --:--:-- 1105k\n",
      "100 35.3M    0 35.3M    0     0   804k      0 --:--:--  0:00:44 --:--:-- 1104k\n",
      "100 36.1M    0 36.1M    0     0   803k      0 --:--:--  0:00:46 --:--:-- 1044k\n",
      "100 36.7M    0 36.7M    0     0   801k      0 --:--:--  0:00:46 --:--:--  971k\n",
      "100 38.0M    0 38.0M    0     0   812k      0 --:--:--  0:00:48 --:--:-- 1003k\n",
      "100 39.1M    0 39.1M    0     0   818k      0 --:--:--  0:00:48 --:--:-- 1002k\n",
      "100 40.1M    0 40.1M    0     0   823k      0 --:--:--  0:00:49 --:--:--  996k\n",
      "100 40.9M    0 40.9M    0     0   824k      0 --:--:--  0:00:50 --:--:-- 1023k\n",
      "x drugsComTest_raw.tsv\n",
      "x drugsComTrain_raw.tsv\n"
     ]
    }
   ],
   "source": [
    "!curl -O \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!tar -xvf drugsCom_raw.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSV (Tab-Separated Values) functions similarly to CSV (Comma-Separated Values) but employs tabs instead of commas as the separator. To load TSV files, you can use the csv loading script and indicate the delimiter argument within the load_dataset() function, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
    "# \\t is the tab character in Python\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One effective practice during data analysis is to extract a small random sample to gain a preliminary understanding of the data structure. In ðŸ¤— Datasets, generating a random sample involves combining the `Dataset.shuffle()` and `Dataset.select()` functions in a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# Peek at the first few examples\n",
    "drug_sample[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
