{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Library**\n",
    "\n",
    "In this chapter, you'll delve deeper into the capabilities of the ðŸ¤— Datasets library. Here are some of the key questions you'll explore:\n",
    "\n",
    "1. How to handle datasets not available on the Hugging Face Hub?\n",
    "2. Techniques for slicing, dicing, and working with datasets, including using Pandas.\n",
    "3. Handling large datasets that might overwhelm your system's RAM.\n",
    "4. Understanding concepts like memory mapping and Apache Arrow.\n",
    "5. Creating custom datasets and contributing them to the Hugging Face Hub.\n",
    "\n",
    "Let's embark on this journey to enhance your understanding of ðŸ¤— Datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What should I do if my dataset isn't available on the Hugging Face Hub?\n",
    "\n",
    "You've learned how to utilize the Hugging Face Hub to fetch datasets, but there will be instances where you need to work with data stored locally on your laptop or on a remote server. In this section, we'll explore how ðŸ¤— Datasets can be employed to load datasets that aren't accessible on the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with local and remote datasets\n",
    "\n",
    "ðŸ¤— Datasets simplifies the loading of local and remote datasets by providing loading scripts for various common data formats. Here are examples of loading scripts for different data formats:\n",
    "\n",
    "- CSV & TSV: `load_dataset(\"csv\", data_files=\"my_file.csv\")`\n",
    "- Text files: `load_dataset(\"text\", data_files=\"my_file.txt\")`\n",
    "- JSON & JSON Lines: `load_dataset(\"json\", data_files=\"my_file.jsonl\")`\n",
    "- Pickled DataFrames: `load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")`\n",
    "\n",
    "The above illustrates that for each data format, specifying the type of loading script in the `load_dataset()` function is sufficient. Additionally, the `data_files` argument is used to provide the path to one or more files. Let's begin by loading a dataset from local files, and subsequently, we'll explore how to achieve the same with remote files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a local dataset\n",
    "\n",
    "For this example weâ€™ll use the SQuAD-it dataset, which is a large-scale dataset for question answering in Italian.\n",
    "\n",
    "The training and test splits are hosted on GitHub, so we can download them using the blow link:\n",
    "\n",
    "https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "\n",
    "https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
    "\n",
    "Once you've downloaded them, unzip the files. You can see the compressed files has SQuAD_it-train.json and SQuAD_it-test.json, and that the data is stored in the JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a JSON file using the `load_dataset()` function involves specifying whether the dataset is in standard JSON format (resembling a nested dictionary) or JSON Lines format (JSON separated by lines). In datasets like SQuAD-it, the information is stored in a nested structure, often with text contained within a specific field. To load this dataset correctly, we'd specify the `field` argument to accommodate this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading datasets from local files, the default behavior is to generate a DatasetDict object containing at least one split, typically the train split. To verify this, you can inspect the `squad_it_dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output displays the count of rows along with the column names present in the training set. You can explore individual examples by selecting one from the train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_it_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the right approach! Having both the train and test splits within a single DatasetDict object allows for more efficient handling. Mapping each split name to its respective file using the data_files argument ensures the inclusion of both splits in a unified dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having both splits within a unified object facilitates uniform preprocessing across the entire dataset, ensuring consistency in the applied transformations or cleaning methods.\n",
    "\n",
    "Datasets simplifies the process by handling file decompression automatically. Using compressed files directly in the `data_files` argument streamlines the loading process without the need for pre-decompression steps. This means we could have skipped the process of unzipping hte file manually by pointing the `data_files` argument directly to the compressed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether it's ZIP, TAR, or other common compression formats, ðŸ¤— Datasets conveniently handles the decompression process upon loading, ensuring ease of use when working with compressed files directly in the `data_files` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a remote dataset\n",
    "\n",
    "When handling remote datasets, the process remains straightforward. Instead of directing the `data_files` argument to local paths, you simply assign it the URLs where the remote files are located. For instance, in the case of the SQuAD-it dataset residing on GitHub, the `data_files` parameter can point directly to the URLs hosting the SQuAD_it-*.json.gz files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This retrieves the identical DatasetDict object we previously obtained, eliminating the need for manual downloading and decompression of the SQuAD_it-*.json.gz files. With this dataset at hand, let's delve into diverse data manipulation techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toying with data subsets\n",
    "\n",
    "In this section, we'll explore various techniques for slicing and dicing data using the ðŸ¤— Datasets library. We'll cover operations like selecting specific columns, filtering rows based on conditions, and shuffling the dataset\n",
    "\n",
    "Beyond Dataset.map(), ðŸ¤— Datasets offers a range of methods to manage datasets. These functions empower you to filter rows, select columns, shuffle data, and more. Let's explore some of these to enhance our dataset manipulations.\n",
    "\n",
    "In this instance, we'll work with the Drug Review Dataset available on the UC Irvine Machine Learning Repository. It includes patient reviews concerning different drugs, along with the treated condition and a 10-star rating reflecting patient satisfaction.\n",
    "\n",
    "First we need to download and extract the data\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "100 94133    0 94133    0     0  22839      0 --:--:--  0:00:04 --:--:-- 22847\n",
      "100  787k    0  787k    0     0   156k      0 --:--:--  0:00:05 --:--:--  156k\n",
      "100 1823k    0 1823k    0     0   304k      0 --:--:--  0:00:05 --:--:--  304k\n",
      "100 2875k    0 2875k    0     0   410k      0 --:--:--  0:00:07 --:--:--  525k\n",
      "100 3807k    0 3807k    0     0   475k      0 --:--:--  0:00:08 --:--:--  764k\n",
      "100 4747k    0 4747k    0     0   527k      0 --:--:--  0:00:08 --:--:--  954k\n",
      "100 5455k    0 5455k    0     0   545k      0 --:--:--  0:00:09 --:--:--  938k\n",
      "100 6223k    0 6223k    0     0   565k      0 --:--:--  0:00:10 --:--:--  880k\n",
      "100 7087k    0 7087k    0     0   590k      0 --:--:--  0:00:11 --:--:--  844k\n",
      "100 8195k    0 8195k    0     0   629k      0 --:--:--  0:00:13 --:--:--  875k\n",
      "100 8959k    0 8959k    0     0   640k      0 --:--:--  0:00:13 --:--:--  842k\n",
      "100 9943k    0 9943k    0     0   662k      0 --:--:--  0:00:15 --:--:--  893k\n",
      "100 10.6M    0 10.6M    0     0   681k      0 --:--:--  0:00:15 --:--:--  934k\n",
      "100 11.6M    0 11.6M    0     0   701k      0 --:--:--  0:00:16 --:--:--  968k\n",
      "100 12.5M    0 12.5M    0     0   716k      0 --:--:--  0:00:17 --:--:--  942k\n",
      "100 14.0M    0 14.0M    0     0   756k      0 --:--:--  0:00:18 --:--:-- 1082k\n",
      "100 14.5M    0 14.5M    0     0   745k      0 --:--:--  0:00:19 --:--:--  997k\n",
      "100 15.3M    0 15.3M    0     0   749k      0 --:--:--  0:00:21 --:--:--  968k\n",
      "100 16.2M    0 16.2M    0     0   754k      0 --:--:--  0:00:21 --:--:--  932k\n",
      "100 16.9M    0 16.9M    0     0   753k      0 --:--:--  0:00:22 --:--:--  884k\n",
      "100 17.5M    0 17.5M    0     0   750k      0 --:--:--  0:00:24 --:--:--  724k\n",
      "100 18.2M    0 18.2M    0     0   748k      0 --:--:--  0:00:24 --:--:--  762k\n",
      "100 18.9M    0 18.9M    0     0   747k      0 --:--:--  0:00:25 --:--:--  737k\n",
      "100 19.8M    0 19.8M    0     0   751k      0 --:--:--  0:00:27 --:--:--  742k\n",
      "100 20.5M    0 20.5M    0     0   749k      0 --:--:--  0:00:27 --:--:--  735k\n",
      "100 21.1M    0 21.1M    0     0   747k      0 --:--:--  0:00:28 --:--:--  737k\n",
      "100 21.8M    0 21.8M    0     0   744k      0 --:--:--  0:00:29 --:--:--  721k\n",
      "100 22.4M    0 22.4M    0     0   741k      0 --:--:--  0:00:30 --:--:--  708k\n",
      "100 23.3M    0 23.3M    0     0   745k      0 --:--:--  0:00:31 --:--:--  711k\n",
      "100 24.2M    0 24.2M    0     0   752k      0 --:--:--  0:00:33 --:--:--  769k\n",
      "100 25.1M    0 25.1M    0     0   756k      0 --:--:--  0:00:33 --:--:--  804k\n",
      "100 25.8M    0 25.8M    0     0   757k      0 --:--:--  0:00:34 --:--:--  835k\n",
      "100 26.6M    0 26.6M    0     0   759k      0 --:--:--  0:00:35 --:--:--  871k\n",
      "100 27.3M    0 27.3M    0     0   757k      0 --:--:--  0:00:36 --:--:--  833k\n",
      "100 28.0M    0 28.0M    0     0   755k      0 --:--:--  0:00:37 --:--:--  770k\n",
      "100 28.9M    0 28.9M    0     0   758k      0 --:--:--  0:00:39 --:--:--  775k\n",
      "100 29.9M    0 29.9M    0     0   766k      0 --:--:--  0:00:40 --:--:--  830k\n",
      "100 31.0M    0 31.0M    0     0   774k      0 --:--:--  0:00:40 --:--:--  883k\n",
      "100 32.0M    0 32.0M    0     0   781k      0 --:--:--  0:00:42 --:--:--  956k\n",
      "100 33.1M    0 33.1M    0     0   790k      0 --:--:--  0:00:42 --:--:-- 1054k\n",
      "100 34.2M    0 34.2M    0     0   797k      0 --:--:--  0:00:43 --:--:-- 1105k\n",
      "100 35.3M    0 35.3M    0     0   804k      0 --:--:--  0:00:44 --:--:-- 1104k\n",
      "100 36.1M    0 36.1M    0     0   803k      0 --:--:--  0:00:46 --:--:-- 1044k\n",
      "100 36.7M    0 36.7M    0     0   801k      0 --:--:--  0:00:46 --:--:--  971k\n",
      "100 38.0M    0 38.0M    0     0   812k      0 --:--:--  0:00:48 --:--:-- 1003k\n",
      "100 39.1M    0 39.1M    0     0   818k      0 --:--:--  0:00:48 --:--:-- 1002k\n",
      "100 40.1M    0 40.1M    0     0   823k      0 --:--:--  0:00:49 --:--:--  996k\n",
      "100 40.9M    0 40.9M    0     0   824k      0 --:--:--  0:00:50 --:--:-- 1023k\n",
      "x drugsComTest_raw.tsv\n",
      "x drugsComTrain_raw.tsv\n"
     ]
    }
   ],
   "source": [
    "!curl -O \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!tar -xvf drugsCom_raw.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSV (Tab-Separated Values) functions similarly to CSV (Comma-Separated Values) but employs tabs instead of commas as the separator. To load TSV files, you can use the csv loading script and indicate the delimiter argument within the load_dataset() function, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
    "# \\t is the tab character in Python\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One effective practice during data analysis is to extract a small random sample to gain a preliminary understanding of the data structure. In ðŸ¤— Datasets, generating a random sample involves combining the `Dataset.shuffle()` and `Dataset.select()` functions in a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# Peek at the first few examples\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here is a rephrased version of the text:\n",
    "\n",
    "To ensure consistent results, we've fixed the seed in the `Dataset.shuffle()` function. Since `Dataset.select()` requires an iterable of indices, we've passed `range(1000)` to extract the first 1,000 samples from the shuffled dataset. This initial sample reveals a few peculiarities in our dataset:\n",
    "\n",
    "- The `Unnamed: 0` column appears to be an anonymized patient ID.\n",
    "- The `condition` column contains a combination of uppercase and lowercase labels.\n",
    "- The reviews vary in length and include a mix of Python line separators (`\\r\\n`) and HTML character codes like `&#039;`.\n",
    "\n",
    "To confirm our hypothesis that the `Unnamed: 0` column represents anonymized patient IDs, we can use the `Dataset.unique()` function to check if the number of IDs matches the number of rows in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in drug_dataset.keys():\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis about the `Unnamed: 0` column being anonymized patient IDs seems valid. Let's improve the dataset's clarity by renaming the `Unnamed: 0` column to something more meaningful. We can use the `DatasetDict.rename_column()` function to modify the column name in both splits simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
    ")\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the tokenization process discussed in Chapter 3, let's standardize all the `condition` labels using `Dataset.map()`. We can define a simple function that can be applied to all rows in each split of `drug_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_condition(example):\n",
    "    return {\"condition\": example[\"condition\"].lower()}\n",
    "\n",
    "\n",
    "drug_dataset.map(lowercase_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we've encountered an issue with our mapping function. The error indicates that some values in the `condition` column are `None`, which cannot be lowercased because they are not strings. To handle this, we can remove these rows using `Dataset.filter()`, which operates similarly to `Dataset.map()` and takes a function that receives a single sample from the dataset. Instead of defining an explicit function like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nones(x):\n",
    "    return x[\"condition\"] is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of writing an explicit function like `filter_nones` and then calling `drug_dataset.filter(filter_nones)`, we can accomplish the same task in a single line using a lambda function. In Python, lambda functions are concise functions that can be defined without explicitly naming them. They follow the general structure:\n",
    "\n",
    "        lambda <arguments> : <expression>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lambda` keyword is a special term in Python that introduces anonymous functions. The `<arguments>` section is a list or set of comma-separated values that represent the function's inputs. The `<expression>` part specifies the operations to be performed. For instance, we can define a simple lambda function that squares a number using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda x : x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize this function for a given input, it needs to be enclosed in parentheses along with the input itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x: x * x)(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner, lambda functions can accommodate multiple arguments by separating them with commas. For instance, we can calculate the area of a triangle using the following lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda base, height: 0.5 * base * height)(4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda functions prove useful when you need to create concise, disposable functions (for more details, we recommend reading Andre Burgaud's exceptional Real Python tutorial). Within the context of ðŸ¤— Datasets, lambda functions allow us to define straightforward map and filter operations. Let's leverage this approach to remove the `None` entries from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `None` entries have been eliminated, we can proceed with normalizing our `condition` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "# Check that lowercasing worked\n",
    "drug_dataset[\"train\"][\"condition\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! After normalizing the labels, let's turn our attention to cleaning up the reviews themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new columns\n",
    "\n",
    "When dealing with customer reviews, it's advisable to examine the number of words in each review. A review could range from a single word like \"Great!\" to a lengthy essay spanning thousands of words. Depending on the specific application, you may need to handle these extremes differently. To determine the number of words in each review, we'll employ a basic approach that involves splitting each text by whitespace.\n",
    "\n",
    "Let's define a straightforward function that calculates the word count for each review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our `lowercase_condition()` function, `compute_review_length()` yields a dictionary whose key doesn't match any of the column names in the dataset. Consequently, when `compute_review_length()` is passed to `Dataset.map()`, it will be applied to every row in the dataset, generating a new `review_length` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "# Inspect the first training example\n",
    "drug_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As anticipated, our training set now includes a `review_length` column. To examine the extreme values of this new column, we can sort it using `Dataset.sort()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset[\"train\"].sort(\"review_length\")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our suspicions were confirmed. Some reviews consist of only a single word, which, while acceptable for sentiment analysis, would not provide sufficient information for the task of predicting the condition.\n",
    "\n",
    "An alternative method for adding new columns to a dataset is the `Dataset.add_column()` function. This function enables you to supply the column as a Python list or NumPy array, making it useful in scenarios where `Dataset.map()` is not the optimal choice for your analysis.\n",
    "\n",
    "Let's employ the `Dataset.filter()` function to eliminate reviews with less than 30 words. Similar to our approach for handling the `condition` column, we can filter out extremely short reviews by requiring that the reviews' length exceed this threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident, this process has eliminated approximately 15% of the reviews from our original training and test sets.\n",
    "\n",
    "The final step involves addressing the presence of HTML character codes in our reviews. Python's `html` module provides a convenient tool for unescaping these characters, as demonstrated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm a transformer called BERT\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "text = \"I&#039;m a transformer called BERT\"\n",
    "html.unescape(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll leverage `Dataset.map()` to unescape all HTML characters within our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The transformative power of the `map()` method\n",
    "\n",
    "The `Dataset.map()` method offers a `batched` argument that, when set to `True`, instructs it to send a batch of examples to the map function simultaneously. The batch size is configurable with a default value of 1,000. For instance, the previous map function that unescaped all HTML characters took a noticeable amount of time to execute (the progress bars display the elapsed time). We can expedite this process by processing multiple elements concurrently using a list comprehension.\n",
    "\n",
    "When `batched=True`, the function receives a dictionary containing the dataset's fields, but each value is now a list of values rather than a single value. The return value of `Dataset.map()` should remain consistent: a dictionary with the fields we want to update or add to our dataset, along with a list of values. Here's an alternative method for unescaping all HTML characters using `batched=True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_drug_dataset = drug_dataset.map(\n",
    "    lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running this code in a notebook, you'll observe that this command executes significantly faster than the previous one. This improvement is not due to the reviews already being HTML-unescaped; re-running the instruction from the previous section (without `batched=True`) will yield the same execution time as before. The reason for this performance gain is that list comprehensions are typically faster than executing the same code in a `for` loop. Additionally, accessing numerous elements simultaneously rather than one at a time contributes to the performance improvement.\n",
    "\n",
    "Employing `Dataset.map()` with `batched=True` will prove crucial in harnessing the speed of the \"fast\" tokenizers, which excel at tokenizing large text collections efficiently. For instance, to tokenize all drug reviews using a fast tokenizer, we could utilize a function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you learned in Chapter 3, the tokenizer can handle one or multiple examples, enabling us to utilize this function with or without `batched=True`. Let's seize this opportunity to compare the performance of these different approaches. In a notebook environment, you can time a single-line instruction by adding `%time` before the line of code you want to measure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also time an entire cell by placing %%time at the beginning of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results we obtained with and without batching, with a fast and a slow tokenizer:\n",
    "\n",
    "![](2023-11-23-13-28-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that employing a fast tokenizer with the `batched=True` option is approximately 30 times faster than its slow counterpart without batchingâ€”an incredible performance gain! This remarkable speedup stems from the use of Rust, a language that facilitates code parallelization, in the background tokenization process. Owing to these advantages, fast tokenizers are the default choice when using `AutoTokenizer` (and the reason behind their name).\n",
    "\n",
    "Parallelization is also the driving force behind the significant speedup, nearly 6 times faster, achieved by the fast tokenizer with batching. Parallelizing a single tokenization operation is not feasible, but when tokenizing multiple texts simultaneously, the execution can be distributed across multiple processes, each handling its assigned texts. This parallel execution model enables the impressive performance gain observed.\n",
    "\n",
    "The `Dataset.map()` function also offers parallelization capabilities. While these capabilities don't leverage Rust, they won't allow a slow tokenizer to match the performance of a fast one, but they can still provide significant benefits, particularly when using a tokenizer that lacks a fast version. To enable multiprocessing, utilize the `num_proc` argument and specify the desired number of processes in your `Dataset.map()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
    "\n",
    "\n",
    "def slow_tokenize_function(examples):\n",
    "    return slow_tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with timing to determine the optimal number of processes to use; in our case, 8 seemed to yield the most significant speed gain. Here's a comparison of the performance with and without multiprocessing:\n",
    "\n",
    "![](2023-11-23-13-25-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While multiprocessing significantly improved the performance of the slow tokenizer, it also enhanced the fast tokenizer's performance. However, this improvement is not always guaranteed. For values of `num_proc` other than 8, our tests revealed that using `batched=True` without multiprocessing was faster. As a general recommendation, we advise against using Python multiprocessing for fast tokenizers with `batched=True`.\n",
    "\n",
    "The versatility of the `Dataset.map()` method with `batched=True` is truly remarkable. It not only simplifies processing large datasets but also enables modifying the number of elements in the dataset. This capability proves particularly valuable in scenarios where you want to generate multiple training features from a single example.\n",
    "\n",
    "In the realm of machine learning, an \"example\" typically refers to the collection of \"features\" provided to the model for training. In certain contexts, these features correspond to the columns in a \"Dataset.\" However, in other scenarios, such as here and in question answering, multiple features can be derived from a single example and reside within a single column.\n",
    "\n",
    "Letâ€™s have a look at how it works! Here we will tokenize our examples and truncate them to a maximum length of 128, but we will ask the tokenizer to return all the chunks of the texts instead of just the first one. This can be done with return_overflowing_tokens=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s test this on one example before using Dataset.map() on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
    "[len(inp) for inp in result[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, our initial example in the training set was divided into two features due to exceeding the specified maximum token length during tokenization. The resulting features have lengths of 128 and 49 tokens, respectively. Let's now apply this process to all elements of the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! It appears that something went wrong during the tokenization process. Upon examining the error message, we discover a discrepancy in the lengths of two columns: one column has a length of 1,463, while the other has a length of 1,000. If you've reviewed the `Dataset.map()` documentation, you might recall that it specifies the number of samples passed to the mapping function. In this case, 1,000 examples were provided to the function, resulting in 1,463 new features, leading to a shape error.\n",
    "\n",
    "The underlying issue lies in the attempt to combine two datasets of different sizes. The `drug_dataset` columns will have a specific number of examples (in our case, 1,000, as indicated by the error message), while the `tokenized_dataset` we're constructing will have more examples (1,463, as mentioned in the error message; this exceeds 1,000 because we're splitting lengthy reviews into multiple examples using `return_overflowing_tokens=True`). This mismatch is incompatible with a `Dataset`, so we either need to remove the columns from the original dataset or ensure they have the same size as in the new dataset. The former approach can be achieved using the `remove_columns` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = drug_dataset.map(\n",
    "    tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the columns, the tokenization process proceeds without errors. We can verify that the new dataset contains significantly more elements than the original dataset by comparing their respective lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_dataset[\"train\"]), len(drug_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned earlier, addressing the mismatched length issue can also be achieved by adjusting the size of the old columns to match that of the new ones. To accomplish this, we'll utilize the `overflow_to_sample_mapping` field that the tokenizer returns when `return_overflowing_tokens=True` is set. This field provides a mapping from a new feature index to the index of the sample it originated from. Leveraging this mapping, we can associate each key in our original dataset with a list of values of the appropriate size by replicating the values of each example as many times as it produces new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_split(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    # Extract mapping between new and old indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it works with Dataset.map() without us needing to remove the old columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach yields the same number of training features as the previous method, but it preserves all the original fields. If you require these fields for post-processing steps after applying your model, this approach might be preferable.\n",
    "\n",
    "As you have witnessed, ðŸ¤— Datasets offers a versatile toolkit for preprocessing datasets in various ways. While the processing functions provided by ðŸ¤— Datasets will address most of your model training needs, there may be instances where you need to transition to Pandas to access more advanced features, such as `DataFrame.groupby()` or high-level visualization APIs. Fortunately, ðŸ¤— Datasets is designed for seamless interoperability with libraries like Pandas, NumPy, PyTorch, TensorFlow, and JAX. Let's explore how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Datasets to DataFrames and back\n",
    "\n",
    "To facilitate conversion between various third-party libraries, ðŸ¤— Datasets offers the `Dataset.set_format()` function. This function exclusively alters the dataset's output format, allowing you to seamlessly switch between formats without impacting the underlying data format, which is Apache Arrow. The formatting is applied directly to the dataset. To illustrate this, let's convert our dataset to Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we access elements of the dataset we get a pandas.DataFrame instead of a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s create a pandas.DataFrame for the whole training set by selecting all the elements of drug_dataset[\"train\"]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = drug_dataset[\"train\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the technical level, `Dataset.set_format()` alters the return format for the dataset's `__getitem__()` method. This implies that when attempting to create a new object like `train_df` from a `Dataset` in the `\"pandas\"` format, the entire dataset must be sliced to obtain a `pandas.DataFrame`. You can independently verify that the type of `drug_dataset[\"train\"]` remains `Dataset`, regardless of the output format.\n",
    "\n",
    "Once the dataset is converted to the \"pandas\" format, you can leverage the full range of Pandas functionalities. For instance, you can employ elegant chaining to calculate the class distribution within the \"condition\" entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = (\n",
    "    train_df[\"condition\"]\n",
    "    .value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n",
    ")\n",
    "frequencies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing your Pandas analysis, you can seamlessly convert the modified DataFrame back into a `Dataset` object using the `Dataset.from_pandas()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "freq_dataset = Dataset.from_pandas(frequencies)\n",
    "freq_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our exploration of the diverse preprocessing capabilities offered by ðŸ¤— Datasets. To finalize this section, let's establish a validation set to prepare the dataset for training a classifier. Before proceeding, we'll revert the output format of `drug_dataset` from `\"pandas\"` to `\"arrow\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a validation set\n",
    "\n",
    "While we have access to a test set for evaluation, it's a prudent practice to preserve its integrity and create a distinct validation set during the development phase. Once you're satisfied with the performance of your models on the validation set, you can perform a final sanity check on the test set. This approach helps alleviate the risk of overfitting to the test set and deploying a model that performs poorly on real-world data.\n",
    "\n",
    "ðŸ¤— Datasets offers the `Dataset.train_test_split()` function, which draws inspiration from the popular scikit-learn functionality. Let's utilize this function to divide our training set into \"train\" and \"validation\" splits (the `seed` argument is set for reproducibility):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a dataset\n",
    "\n",
    "While ðŸ¤— Datasets automatically caches every downloaded dataset and the transformations applied to it, there may be instances where you need to explicitly save a dataset to disk (for example, to prevent data loss in case the cache is cleared). As illustrated in the table below, ðŸ¤— Datasets offers three primary functions for saving datasets in various formats:\n",
    "\n",
    "![](2023-11-23-15-31-26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, letâ€™s save our cleaned dataset in the Arrow format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset_clean.save_to_disk(\"drug-reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a directory with the following structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            drug-reviews/\n",
    "            â”œâ”€â”€ dataset_dict.json\n",
    "            â”œâ”€â”€ test\n",
    "            â”‚   â”œâ”€â”€ dataset.arrow\n",
    "            â”‚   â”œâ”€â”€ dataset_info.json\n",
    "            â”‚   â””â”€â”€ state.json\n",
    "            â”œâ”€â”€ train\n",
    "            â”‚   â”œâ”€â”€ dataset.arrow\n",
    "            â”‚   â”œâ”€â”€ dataset_info.json\n",
    "            â”‚   â”œâ”€â”€ indices.arrow\n",
    "            â”‚   â””â”€â”€ state.json\n",
    "            â””â”€â”€ validation\n",
    "                â”œâ”€â”€ dataset.arrow\n",
    "                â”œâ”€â”€ dataset_info.json\n",
    "                â”œâ”€â”€ indices.arrow\n",
    "                â””â”€â”€ state.json\n",
    "\n",
    "\n",
    "Each split is linked to its own `dataset.arrow` table, along with some metadata stored in `dataset_info.json` and `state.json`. The Arrow format can be conceptualized as an advanced table of columns and rows, optimized for developing high-performance applications that handle and transfer large datasets.\n",
    "\n",
    "To load a saved dataset, utilize the `load_from_disk()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"drug-reviews\")\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When saving datasets in CSV or JSON formats, each split must be stored as an individual file. One approach to achieve this is to iterate through the keys and values in the `DatasetDict` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"drug-reviews-{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves each split in JSON Lines format, where each row in the dataset is stored as a single line of JSON. Hereâ€™s what the first example looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 drug-reviews-train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the techniques from section 2 to load the JSON files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": \"drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Excellent! We have successfully explored data wrangling techniques with ðŸ¤— Datasets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
