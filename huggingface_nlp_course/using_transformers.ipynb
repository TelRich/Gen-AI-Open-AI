{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Using Transformers ü§ó:**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "ü§ó Transformers, often referred to as Hugging Face Transformers, is a popular library that provides easy access to state-of-the-art natural language processing (NLP) models. This chapter introduces you to the key concepts and tools within the ü§ó Transformers library, allowing you to leverage powerful NLP models for various tasks.\n",
    "\n",
    "Behind the Pipeline:\n",
    "\n",
    "Learn how the ü§ó Transformers library simplifies NLP tasks with the concept of pipelines. This section delves into the inner workings of pipelines, which automate common NLP workflows, and explores how to set up and configure pipelines for various tasks.\n",
    "\n",
    "Models:\n",
    "\n",
    "Discover the wide array of pre-trained NLP models available in the ü§ó Transformers library. These models, based on transformer architectures, are capable of tasks like text classification, question answering, translation, summarization, and more. You'll gain insights into selecting and using the right model for your specific needs.\n",
    "\n",
    "Tokenizers:\n",
    "\n",
    "Tokenization is a crucial aspect of NLP, and ü§ó Transformers offers various tokenization tools to preprocess text data effectively. This section explores tokenizers, their configurations, and how to tokenize text for model input.\n",
    "\n",
    "Handling Multiple Sequences:\n",
    "\n",
    "NLP tasks often require handling multiple sequences of data, such as translating text from one language to another. This section guides you on working with multiple sequences in the ü§ó Transformers library, ensuring you can manage complex tasks seamlessly.\n",
    "\n",
    "Putting It All Together:\n",
    "\n",
    "In this section, you'll put your knowledge to the test by applying the concepts learned in the previous sections. You'll walk through practical examples of using the ü§ó Transformers library to accomplish specific NLP tasks, demonstrating how to build end-to-end solutions.\n",
    "\n",
    "Basic Usage Completed:\n",
    "\n",
    "By the end of this chapter, you will have a strong foundation in using the ü§ó Transformers library for NLP tasks. You'll be prepared to tackle various real-world NLP challenges with the help of state-of-the-art models and tools.\n",
    "\n",
    "End-of-Chapter Quiz:\n",
    "\n",
    "Test your understanding of the key concepts covered in this chapter with an end-of-chapter quiz. This quiz will help reinforce your knowledge and ensure you're ready to apply what you've learned in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Behind the Pipeline**\n",
    "\n",
    "To harness the power of transformer models for natural language processing (NLP) tasks, it's crucial to understand the mechanics behind the pipeline. The pipeline is a high-level interface that streamlines the process of applying transformers to text data. It automates many common NLP tasks, making it easier to work with these models effectively. Let's dive into the key components of the pipeline:\n",
    "\n",
    "1. **Tokenization**: The first step in the pipeline is tokenization. This process takes raw text and breaks it down into smaller units called tokens. Tokens are typically words or subwords, and they are the basic units of text that the model processes. Tokenization ensures that the text is structured for the model's input.\n",
    "\n",
    "2. **Model Loading**: After tokenization, the pipeline loads a pre-trained transformer model. These models have been trained on large text corpora and have learned to understand the context and relationships between words and phrases. Examples of popular transformer models include BERT, GPT-2, and RoBERTa.\n",
    "\n",
    "3. **Inference**: The pipeline uses the loaded model to make inferences on the tokenized input text. Depending on the specific NLP task, the model may generate predictions, classifications, or other outputs. This step leverages the model's understanding of context to process and analyze the text effectively.\n",
    "\n",
    "4. **Output**: The pipeline returns the model's output, which can vary depending on the task. For instance, if the task is text classification, the output may be a label or category. If the task is text generation, the output may be a generated text sequence. The output is designed to be easily accessible and ready for further processing or analysis.\n",
    "\n",
    "5. **Post-Processing**: In many cases, the pipeline also includes post-processing steps. This can involve converting model outputs into human-readable formats, such as decoding generated tokens or mapping prediction scores to class labels.\n",
    "\n",
    "The pipeline encapsulates these steps, allowing you to work with transformer models in a more user-friendly and efficient way. This simplification is particularly valuable when you need to perform various NLP tasks without delving into the low-level details of model loading, tokenization, and post-processing.\n",
    "\n",
    "By understanding the pipeline, you can leverage transformer models for a wide range of NLP tasks, from text classification and sentiment analysis to question answering and text generation. The pipeline provides a powerful and accessible interface for applying these state-of-the-art models to real-world text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing with a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"HuggingFace provides amazing NLP resources.\",\n",
    "    \"I love learning about natural language processing!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high-dimensional vector?\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "Batch size: The number of sequences processed at a time (2 in our example).\n",
    "Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "Hidden size: The vector dimension of each model input.\n",
    "It is said to be ‚Äúhigh dimensional‚Äù because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more).\n",
    "\n",
    "We can see this if we feed the inputs we preprocessed to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model heads: Making sense out of numbers\n",
    "\n",
    "The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:\n",
    "\n",
    "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won‚Äôt actually use the AutoModel class, but AutoModelForSequenceClassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postprocessing the output\n",
    "\n",
    "The values we get as output from our model don‚Äôt necessarily make sense by themselves. Let‚Äôs take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer (all ü§ó Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can conclude that the model predicted the following:\n",
    "\n",
    "First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
    "Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005\n",
    "We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Models**\n",
    "\n",
    "In this section, we'll delve deeper into the process of creating and utilizing a model. We'll be making use of the AutoModel class, which is quite convenient when you need to create an instance of any model from a checkpoint.\n",
    "\n",
    "The AutoModel class, along with its related classes, serves as a straightforward wrapper for the diverse array of models available in the library. It's a clever wrapper because it can automatically deduce the appropriate model architecture for your checkpoint and then instantiate a model with that specific architecture.\n",
    "\n",
    "However, if you happen to know the exact model type you wish to use, you can opt to use the class that explicitly defines its architecture. Let's explore this further with a BERT model as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Transformer\n",
    "\n",
    "The first thing we‚Äôll need to do to initialize a BERT model is load a configuration object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration contains many attributes that are used to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we haven't explored the specific functionality of all these attributes in detail yet, you might already be familiar with some of them. For instance, the `hidden_size` attribute specifies the dimension of the `hidden_states` vector, and `num_hidden_layers` defines the number of layers present in the Transformer model. These are crucial architectural parameters that significantly influence the model's behavior and capacity. As you continue to work with Transformers, you'll gain a deeper understanding of how these attributes impact the model's performance and behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different loading methods\n",
    "\n",
    "Creating a model from the default configuration initializes it with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model in its initial state is possible, but it would produce unintelligible results since it needs to be trained for a specific task. Training the model from scratch on a particular task would indeed be a time-consuming process, requiring a substantial amount of data and having a significant environmental impact, as discussed in Chapter 1.\n",
    "\n",
    "To circumvent the need for such extensive and duplicated efforts, it's crucial to have the capability to share and reuse models that have already undergone training. Loading a pretrained Transformer model is a straightforward process, and you can achieve this by using the `from_pretrained()` method. This method allows you to access and utilize models that have already been trained on various tasks, saving both time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated earlier, we were able to replace the use of BertModel with the equivalent AutoModel class. We'll continue using this approach, as it leads to code that is agnostic to the specific checkpoint, ensuring that your code can seamlessly work with different checkpoints. This flexibility extends even to cases where the architecture might differ, as long as the checkpoint was trained for a similar task, such as sentiment analysis.\n",
    "\n",
    "In the provided code sample, we didn't utilize BertConfig. Instead, we loaded a pretrained model using the identifier \"bert-base-cased.\" This particular model checkpoint was trained by the creators of BERT themselves, and you can find more detailed information about it in its model card.\n",
    "\n",
    "The model is initialized with all the weights from the checkpoint, making it suitable for immediate inference on the tasks it was originally trained for. Additionally, it can be fine-tuned for new tasks. Leveraging pretrained weights in this manner allows you to achieve good results more rapidly compared to training a model from scratch.\n",
    "\n",
    "Furthermore, the weights have been downloaded and cached during this process, so subsequent calls to the from_pretrained() method won't involve redownloading them. The default cache folder is located at ~/.cache/huggingface/transformers, but you can customize this location by setting the HF_HOME environment variable.\n",
    "\n",
    "The identifier used to load the model can be the identifier of any model available on the Model Hub, provided that it is compatible with the BERT architecture. You can find a comprehensive list of available BERT checkpoints on the official Transformers Model Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving methods\n",
    "\n",
    "Saving a model is as easy as loading one ‚Äî we use the save_pretrained() method, which is analogous to the from_pretrained() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves two files to your disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "ls directory_on_my_computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you inspect the `config.json` file, you will find the attributes essential for constructing the model's architecture. This file also includes metadata, such as the source of the checkpoint and the version of ü§ó Transformers used when the checkpoint was last saved.\n",
    "\n",
    "On the other hand, the `pytorch_model.bin` file is referred to as the state dictionary, and it contains all the weights of your model. These two files are tightly interrelated: the configuration file is critical for understanding your model's architecture, while the model weights stored in the state dictionary represent the parameters that define your model. Together, they are fundamental components for loading and using a pretrained model checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Transformer model for inference\n",
    "\n",
    "Now that you've learned how to load and save a model, let's explore using it to make predictions. Transformer models can only process numerical data, which is generated by tokenizers. Before delving into tokenizers, let's first understand what types of inputs the model can accept.\n",
    "\n",
    "Tokenizers play a crucial role in converting inputs into the appropriate tensors for the underlying framework. However, to provide you with a better understanding of the process, let's take a quick look at what needs to be done before sending inputs to the model.\n",
    "\n",
    "For instance, suppose we have a couple of sequences:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer's role is to convert these sequences into vocabulary indices, which are commonly referred to as input IDs. Consequently, each sequence is transformed into a list of numbers. The resulting output is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, what we have now is a list of encoded sequences, essentially a list of lists. However, it's important to note that tensors, which are used in most deep learning frameworks, require rectangular shapes, similar to matrices. Fortunately, this \"array\" is already in a rectangular shape, making it straightforward to convert it into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the tensors as inputs to the model\n",
    "\n",
    "Making use of the tensors with the model is extremely simple ‚Äî we just call the model with the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer model can accept various arguments, but for the basic usage, only the input IDs are essential. We will explore the functionality and use of the other arguments later on. However, before delving into those details, it's important to examine the tokenizers responsible for constructing inputs that a Transformer model can comprehend. These tokenizers play a critical role in preparing the input data for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tokenizers**\n",
    "\n",
    "Tokenization is a fundamental step in natural language processing (NLP) that involves breaking down text into smaller units, called tokens. Tokens are typically words or subwords, and they serve as the basic building blocks for NLP models. Tokenizers are essential for preparing text data for processing by transformer models, and here's why they are important:\n",
    "\n",
    "1. **Handling Text Data**: Text data is inherently unstructured, and tokenization helps convert it into a format that can be processed effectively by NLP models. It breaks text into manageable chunks, making it suitable for analysis.\n",
    "\n",
    "2. **Word and Subword Splitting**: Tokenization splits text into individual words or subwords, which allows models to understand and generate text at a granular level. Subword tokenization is particularly useful for handling languages with complex word formations.\n",
    "\n",
    "3. **Vocabulary Mapping**: Tokenizers maintain a vocabulary or dictionary that maps tokens to numerical IDs. This mapping is crucial because NLP models operate on numerical input. Token IDs are used as inputs to the model, and the model's output is often in the form of token IDs, which are then mapped back to text.\n",
    "\n",
    "4. **Special Tokens**: Tokenizers often include special tokens, such as [CLS] and [SEP], which are used for specific purposes. For example, [CLS] might be used to indicate the start of a text sequence in a classification task, and [SEP] can be used to separate segments of text in various tasks.\n",
    "\n",
    "5. **Segmentation**: Tokenization can handle tasks involving multiple sequences or segments of text. For instance, in machine translation, a tokenizer can segment text into source and target languages.\n",
    "\n",
    "6. **Subword Tokenization**: Subword tokenization is useful for handling languages with a vast vocabulary or for splitting long words into meaningful parts. This technique is particularly helpful for languages like Chinese or for handling domain-specific terminology.\n",
    "\n",
    "7. **Special Tokens**: Tokenizers often include special tokens, such as [CLS] and [SEP], which are used for specific purposes. For example, [CLS] might be used to indicate the start of a text sequence in a classification task, and [SEP] can be used to separate segments of text in various tasks.\n",
    "\n",
    "8. **Pre-processing and Post-processing**: Tokenizers can handle pre-processing tasks like lowercasing, removing punctuation, and handling special characters. They can also perform post-processing, such as converting token IDs back to text.\n",
    "\n",
    "9. **Fine-Tuning**: When fine-tuning transformer models for specific tasks, tokenizers ensure that the text data used for fine-tuning is processed consistently with the pre-trained model's tokenization.\n",
    "\n",
    "10. **Open-Source Tokenizers**: Many open-source tokenizer implementations are available for popular transformer models, such as the Hugging Face Transformers library, which provides access to a variety of tokenizers and pre-trained models.\n",
    "\n",
    "In summary, tokenization is a crucial step in NLP that prepares text data for processing by transformer models. It enables the handling of text at the word or subword level, maintains token-to-ID mappings, and ensures that text data is structured for effective analysis and model input. Different languages and tasks may require specific tokenization strategies to handle their unique characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some examples of tokenization algorithms and address any questions you might have regarding the tokenization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-based\n",
    "\n",
    "Word-based tokenization is one of the most straightforward and commonly used tokenization approaches. It is easy to set up and typically involves a few rules, making it a good choice for many applications. The main objective of word-based tokenization is to break down raw text into individual words and assign a numerical representation to each word. This approach is exemplified in the image below:\n",
    "\n",
    "![](/workspaces/Gen-AI-Open-AI/image/word_base.png)\n",
    "\n",
    "Indeed, there are various methods to split the text into words. One simple approach is to use whitespace as a delimiter, which can be accomplished by applying Python's `split()` function. This method splits the text wherever it encounters spaces, creating a list of words. This basic word-based tokenization can be quite effective for many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-based tokenizers are useful but can have limitations, especially when dealing with extensive vocabularies in languages with numerous words and variations. In such cases, the vocabulary can become very large, with each word being assigned an ID ranging from 0 to the size of the vocabulary. However, this approach has some drawbacks:\n",
    "\n",
    "1. Variations between words: Similar words with slight differences, such as \"dog\" and \"dogs,\" or \"run\" and \"running,\" may be assigned different IDs initially. The model won't inherently recognize the similarity between such words.\n",
    "\n",
    "2. Handling out-of-vocabulary words: To cover an entire language with a word-based tokenizer, you would need an identifier for every word in the language, resulting in an enormous number of tokens. To handle words not in the vocabulary, a special \"unknown\" token, often represented as \"[UNK]\" or something similar, is used. The presence of many unknown tokens can indicate a limitation of the tokenizer, as it signifies a loss of information.\n",
    "\n",
    "To mitigate these issues and reduce the reliance on the unknown token, you can employ a character-based tokenizer, which delves one level deeper into text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-based\n",
    "\n",
    "Character-based tokenizers operate by splitting the text into individual characters, as opposed to words. This approach offers two key advantages:\n",
    "\n",
    "1. Smaller Vocabulary: Character-based tokenizers result in significantly smaller vocabularies compared to word-based tokenizers. The vocabulary size is determined by the number of distinct characters in the language, which is typically much smaller than the number of words.\n",
    "\n",
    "2. Fewer Out-of-Vocabulary Tokens: With character-based tokenization, there are far fewer out-of-vocabulary (unknown) tokens. Since every word can be constructed from characters, the model can represent any word using the characters in its vocabulary.\n",
    "\n",
    "However, character-based tokenization introduces some questions and challenges related to spaces and punctuation. For example:\n",
    "\n",
    "![](/workspaces/Gen-AI-Open-AI/image/character_based.png)\n",
    "\n",
    "Character-based tokenization has its merits, but it also has limitations. One argument against character-based tokenization is that, on an intuitive level, individual characters may not carry as much meaning on their own as whole words. However, this perception can vary depending on the language. For instance, in languages like Chinese, each character can convey more information compared to a character in a Latin language.\n",
    "\n",
    "Additionally, character-based tokenization can lead to a substantial increase in the number of tokens to be processed by the model. While a word is typically represented by a single token in word-based tokenization, it can translate to 10 or more tokens when broken down into characters.\n",
    "\n",
    "To strike a balance between these two approaches, a third technique called subword tokenization is often used. This approach combines the advantages of both word-based and character-based tokenization, making it a versatile choice for handling different languages and text types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword tokenization\n",
    "\n",
    "Subword tokenization algorithms are designed with the principle that commonly used words should remain intact and not be split into smaller subwords, while rare words should be decomposed into meaningful subword units. This approach preserves the meaning of rare words while utilizing frequently occurring subwords, making the tokenization process more efficient and informative.\n",
    "\n",
    "For instance, consider the word \"annoyingly,\" which might be considered a rare word. Using subword tokenization, it could be decomposed into \"annoying\" and \"ly.\" Both \"annoying\" and \"ly\" are likely to appear more frequently as standalone subwords in various contexts. At the same time, the composite meaning of \"annoyingly\" is retained through the combination of \"annoying\" and \"ly.\"\n",
    "\n",
    "Here's an example to illustrate how a subword tokenization algorithm would tokenize the sequence \"Let‚Äôs do tokenization!\":\n",
    "\n",
    "![](/workspaces/Gen-AI-Open-AI/image/subword_tokenization.png)\n",
    "\n",
    "Subword tokenization techniques indeed offer the advantage of preserving semantic meaning while remaining space-efficient. For example, in the given example, \"tokenization\" was split into \"token\" and \"ization,\" both of which have semantic meaning, and only two tokens are needed to represent a long word. This approach allows for relatively good coverage with smaller vocabularies and minimizes the need for unknown tokens.\n",
    "\n",
    "Subword tokenization is particularly valuable in languages with agglutinative features, like Turkish, where complex words can be constructed by concatenating subwords.\n",
    "\n",
    "In addition to the techniques mentioned, such as Byte-level BPE (used in GPT-2), WordPiece (used in BERT), SentencePiece, and Unigram (used in various multilingual models), there are even more tokenization methods available. These techniques provide flexibility in handling different languages, text types, and use cases.\n",
    "\n",
    "With this foundational knowledge of how tokenizers work, you should be well-equipped to start using the API and effectively preprocess text data for various natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and saving\n",
    "\n",
    "Loading and saving tokenizers is straightforward and follows a similar pattern as models, making use of the same two methods: `from_pretrained()` and `save_pretrained()`. These methods handle the loading and saving of the tokenizer's algorithm (akin to the model's architecture) as well as its vocabulary (similar to the model's weights).\n",
    "\n",
    "When loading a tokenizer that matches the checkpoint, the process is much like loading the corresponding model. For example, to load the BERT tokenizer trained with the same checkpoint as BERT, you can use the `BertTokenizer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensures that the tokenizer and the model are in sync, and you can perform tokenization and preprocessing in a consistent manner.\n",
    "\n",
    "Just like the `AutoModel` class, the `AutoTokenizer` class is designed to automatically select the appropriate tokenizer class from the library based on the checkpoint name. This allows you to use it directly with any checkpoint, making it a convenient way to handle tokenization for various models without needing to explicitly specify the tokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tokenizer loaded, you can now use it as demonstrated in the previous section to tokenize and preprocess text data for various natural language processing tasks. This involves converting text into numerical inputs that can be fed into your Transformer model for inference or training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a tokenizer is identical to saving a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore token_type_ids in more detail in Chapter 3, and we will provide an explanation of the attention_mask key a bit later in this discussion. Before diving into those aspects, let's focus on understanding how the input_ids are generated, which requires looking at the intermediate methods of the tokenizer. This will give you a more comprehensive insight into the tokenization process and the generation of input IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "The process of translating text into numerical representations is known as encoding, and it typically involves a two-step procedure: tokenization, followed by the conversion to input IDs.\n",
    "\n",
    "As we've already seen, the first step involves breaking the text into words (or parts of words, punctuation symbols, etc.), known as tokens. This tokenization process can be subject to various rules, which is why it's crucial to instantiate the tokenizer using the model's name to ensure consistency with the rules used during pretraining.\n",
    "\n",
    "The second step is to convert these tokens into numerical values, which allows us to construct a tensor for feeding into the model. This conversion is achieved using the tokenizer's vocabulary, which is downloaded when the tokenizer is instantiated with the `from_pretrained()` method. It's essential to use the same vocabulary that was employed during the model's pretraining.\n",
    "\n",
    "To provide a better understanding of these two steps, we'll explore them separately. Please note that in practice, you would typically call the tokenizer directly on your inputs, and it would handle both tokenization and conversion to input IDs seamlessly. The separation is for illustrative purposes to show the intermediate results of these processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenization**\n",
    "\n",
    "The tokenization process is done by the tokenize() method of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example you provided, `['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']`, demonstrates the tokenization process using a subword tokenizer. In this case, the tokenizer splits words into smaller subword tokens until it finds tokens that can be represented by its vocabulary.\n",
    "\n",
    "For instance, the word \"transformer\" is divided into two tokens: \"Trans\" and \"##former.\" The \"##\" prefix indicates that the token is a continuation of the previous one, and together they represent the original word. This approach allows the tokenizer to efficiently handle a wide range of words and subwords while keeping the vocabulary size manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From tokens to input IDs\n",
    "\n",
    "The conversion to input IDs is handled by the convert_tokens_to_ids() tokenizer method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/TelRich/Gen-AI-Open-AI/blob/main/image/word_base.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Replicate Tokenization and Conversion to Input IDs\n",
    "\n",
    "Replicate the tokenization and conversion to input IDs for the following input sentences:\n",
    "1. \"HuggingFace provides amazing NLP resources.\",\n",
    "2. \"I love learning about natural language processing!\",\n",
    "\n",
    "\n",
    "Using the appropriate tokenizer, tokenize these sentences and convert them into input IDs. Ensure that you obtain the same input IDs as seen earlier in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "\n",
    "Decoding is the process of converting vocabulary indices back into a human-readable text string. This can be accomplished using the `decode()` method, which allows you to reverse the tokenization and obtain the original text from the numerical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that the `decode()` method not only translates the indices back into tokens but also intelligently groups together the tokens that were originally part of the same words, resulting in a coherent and readable sentence. This behavior becomes particularly valuable when working with models that generate new text, such as text generation from a prompt or for sequence-to-sequence tasks like translation or summarization.\n",
    "\n",
    "So far, we've covered the fundamental operations that a tokenizer can perform: tokenization, conversion to IDs, and decoding IDs back into a human-readable string. However, this is just the beginning, and there's much more to explore. In the upcoming section, we'll push our understanding of tokenization to its limits and explore strategies for overcoming challenges in text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling multiple sequences**\n",
    "\n",
    "In the previous section, we covered the basic use case of performing inference on a single sequence of relatively small length. However, as we delve deeper into natural language processing tasks, several questions arise:\n",
    "\n",
    "1. How do we handle multiple sequences?\n",
    "2. How do we handle multiple sequences of different lengths?\n",
    "3. Are vocabulary indices the only inputs that allow a model to work well?\n",
    "4. Is there a practical limit to the length of a sequence that a model can handle effectively?\n",
    "\n",
    "These questions highlight some of the challenges and complexities that can arise when working with text data. In the following sections, we will explore the solutions to these questions using the ü§ó Transformers API, which offers tools and techniques to address these issues and make it easier to work with a wide range of natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models expect a batch of inputs\n",
    "\n",
    "In the previous section, we observed how sequences are translated into lists of numbers. Now, let's take this list of numbers and convert it into a tensor, which can then be sent to the model for processing. This transformation is a crucial step in preparing the data for feeding into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32m/workspaces/Gen-AI-Open-AI/NLP.ipynb Cell 67\u001b[0m line \u001b[0;36m1\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bturbo-space-goggles-57qv554xxj6hvp64/workspaces/Gen-AI-Open-AI/NLP.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(ids)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bturbo-space-goggles-57qv554xxj6hvp64/workspaces/Gen-AI-Open-AI/NLP.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# This line will fail.\u001b[39;00m\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bturbo-space-goggles-57qv554xxj6hvp64/workspaces/Gen-AI-Open-AI/NLP.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m model(input_ids)\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:789\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m    781\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    782\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n",
      "\u001b[1;32m    783\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n",
      "\u001b[1;32m    784\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n",
      "\u001b[1;32m    785\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n",
      "\u001b[1;32m    786\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    787\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n",
      "\u001b[0;32m--> 789\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n",
      "\u001b[1;32m    790\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n",
      "\u001b[1;32m    791\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n",
      "\u001b[1;32m    792\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n",
      "\u001b[1;32m    793\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n",
      "\u001b[1;32m    794\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n",
      "\u001b[1;32m    795\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n",
      "\u001b[1;32m    796\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n",
      "\u001b[1;32m    797\u001b[0m )\n",
      "\u001b[1;32m    798\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n",
      "\u001b[1;32m    799\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:592\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m    590\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m    591\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 592\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
      "\u001b[1;32m    593\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39msize()\n",
      "\u001b[1;32m    594\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/modeling_utils.py:3941\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n",
      "\u001b[1;32m   3938\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[1;32m   3940\u001b[0m \u001b[39m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n",
      "\u001b[0;32m-> 3941\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id \u001b[39min\u001b[39;00m input_ids[:, [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m]]:\n",
      "\u001b[1;32m   3942\u001b[0m     warn_string \u001b[39m=\u001b[39m (\n",
      "\u001b[1;32m   3943\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m   3944\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m   3945\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m   3946\u001b[0m     )\n",
      "\u001b[1;32m   3948\u001b[0m     \u001b[39m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n",
      "\u001b[1;32m   3949\u001b[0m     \u001b[39m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue here is that we attempted to send a single sequence to the model, but ü§ó Transformers models expect input in the form of multiple sentences by default. When using the tokenizer, it doesn't merely convert the list of input IDs into a tensor; it adds an extra dimension on top of it. This extra dimension is critical for correctly formatting the input data for the model. To resolve this issue, we need to ensure that our input is structured correctly to match the model's expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To correctly format the input data for the model, we should add a new dimension to our input. Let's try the conversion again with the extra dimension added to the tensor. This ensures that the input data aligns with the model's expectations for processing multiple sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
