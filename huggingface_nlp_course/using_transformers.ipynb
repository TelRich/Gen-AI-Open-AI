{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Using Transformers ü§ó:**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "ü§ó Transformers, often referred to as Hugging Face Transformers, is a popular library that provides easy access to state-of-the-art natural language processing (NLP) models. This chapter introduces you to the key concepts and tools within the ü§ó Transformers library, allowing you to leverage powerful NLP models for various tasks.\n",
    "\n",
    "Behind the Pipeline:\n",
    "\n",
    "Learn how the ü§ó Transformers library simplifies NLP tasks with the concept of pipelines. This section delves into the inner workings of pipelines, which automate common NLP workflows, and explores how to set up and configure pipelines for various tasks.\n",
    "\n",
    "Models:\n",
    "\n",
    "Discover the wide array of pre-trained NLP models available in the ü§ó Transformers library. These models, based on transformer architectures, are capable of tasks like text classification, question answering, translation, summarization, and more. You'll gain insights into selecting and using the right model for your specific needs.\n",
    "\n",
    "Tokenizers:\n",
    "\n",
    "Tokenization is a crucial aspect of NLP, and ü§ó Transformers offers various tokenization tools to preprocess text data effectively. This section explores tokenizers, their configurations, and how to tokenize text for model input.\n",
    "\n",
    "Handling Multiple Sequences:\n",
    "\n",
    "NLP tasks often require handling multiple sequences of data, such as translating text from one language to another. This section guides you on working with multiple sequences in the ü§ó Transformers library, ensuring you can manage complex tasks seamlessly.\n",
    "\n",
    "Putting It All Together:\n",
    "\n",
    "In this section, you'll put your knowledge to the test by applying the concepts learned in the previous sections. You'll walk through practical examples of using the ü§ó Transformers library to accomplish specific NLP tasks, demonstrating how to build end-to-end solutions.\n",
    "\n",
    "Basic Usage Completed:\n",
    "\n",
    "By the end of this chapter, you will have a strong foundation in using the ü§ó Transformers library for NLP tasks. You'll be prepared to tackle various real-world NLP challenges with the help of state-of-the-art models and tools.\n",
    "\n",
    "End-of-Chapter Quiz:\n",
    "\n",
    "Test your understanding of the key concepts covered in this chapter with an end-of-chapter quiz. This quiz will help reinforce your knowledge and ensure you're ready to apply what you've learned in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Behind the Pipeline**\n",
    "\n",
    "**Behind the Pipeline**\n",
    "\n",
    "To harness the power of transformer models for natural language processing (NLP) tasks, it's crucial to understand the mechanics behind the pipeline. The pipeline is a high-level interface that streamlines the process of applying transformers to text data. It automates many common NLP tasks, making it easier to work with these models effectively. Let's dive into the key components of the pipeline:\n",
    "\n",
    "1. **Tokenization**: The first step in the pipeline is tokenization. This process takes raw text and breaks it down into smaller units called tokens. Tokens are typically words or subwords, and they are the basic units of text that the model processes. Tokenization ensures that the text is structured for the model's input.\n",
    "\n",
    "2. **Model Loading**: After tokenization, the pipeline loads a pre-trained transformer model. These models have been trained on large text corpora and have learned to understand the context and relationships between words and phrases. Examples of popular transformer models include BERT, GPT-2, and RoBERTa.\n",
    "\n",
    "3. **Inference**: The pipeline uses the loaded model to make inferences on the tokenized input text. Depending on the specific NLP task, the model may generate predictions, classifications, or other outputs. This step leverages the model's understanding of context to process and analyze the text effectively.\n",
    "\n",
    "4. **Output**: The pipeline returns the model's output, which can vary depending on the task. For instance, if the task is text classification, the output may be a label or category. If the task is text generation, the output may be a generated text sequence. The output is designed to be easily accessible and ready for further processing or analysis.\n",
    "\n",
    "5. **Post-Processing**: In many cases, the pipeline also includes post-processing steps. This can involve converting model outputs into human-readable formats, such as decoding generated tokens or mapping prediction scores to class labels.\n",
    "\n",
    "The pipeline encapsulates these steps, allowing you to work with transformer models in a more user-friendly and efficient way. This simplification is particularly valuable when you need to perform various NLP tasks without delving into the low-level details of model loading, tokenization, and post-processing.\n",
    "\n",
    "By understanding the pipeline, you can leverage transformer models for a wide range of NLP tasks, from text classification and sentiment analysis to question answering and text generation. The pipeline provides a powerful and accessible interface for applying these state-of-the-art models to real-world text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing with a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high-dimensional vector?\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "Batch size: The number of sequences processed at a time (2 in our example).\n",
    "Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "Hidden size: The vector dimension of each model input.\n",
    "It is said to be ‚Äúhigh dimensional‚Äù because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more).\n",
    "\n",
    "We can see this if we feed the inputs we preprocessed to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model heads: Making sense out of numbers\n",
    "\n",
    "The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
