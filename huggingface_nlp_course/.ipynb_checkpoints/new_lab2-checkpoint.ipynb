{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2 Notebook\n",
    "\n",
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. \n",
    "Preprocessing with a tokenizer. \n",
    "\n",
    "You're to preprocess and display the output of the below sentence for the model. The tokenizer has already been loaded for you.\n",
    "\n",
    "* \"The universe is vast and full of mysteries.\",\n",
    "* \"Exploring the depths of the ocean reveals hidden wonders.\"\n",
    "\n",
    "Set the following parameters:\n",
    "\n",
    "- `padding=True`: This parameter ensures that the sequences are padded to equal lengths if their lengths vary.\n",
    "- `truncation=True`: If any sentence exceeds the maximum length that the model can handle, truncation cuts it to fit within that limit.\n",
    "- `return_tensors=\"pt\"`: This specifies that the tokenizer should return PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. \n",
    "Going through the model\n",
    "\n",
    "Feed the preprocessed text into the model and display the vector output of the transformer. The model has already been loaded for you.\n",
    "\n",
    "What are the batch size, sequence length and hidden size from the vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. \n",
    "Making sense out of the numbers\n",
    "\n",
    "Instead of just extracting features from the input text, we want to actually categorize it as positive or negative. To do this, we need a specialized model head called a \"sequence classification head\" that takes the extracted features and predicts a category for the sentence.\n",
    "\n",
    "While the `AutoModel` class can extract features, it's not designed for classification. Therefore, we'll use `AutoModelForSequenceClassification` instead, which comes with this specialized head built-in.\n",
    "\n",
    "The model has been loaded for you, Pass in the preprocess text and display the logits and it's shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "The output from the model are not probabilities but logits which are unnormalized scores outputted by the last layer of the models. \n",
    "The softmax function is used to convert these logits into probabilities. It takes an array of numbers (logits) as input and returns. Convert those output to probabilities and display them, also display their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. \n",
    "#### Write your conclusions from the model prediction below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    " Replicate Tokenization and Conversion to Input IDs\n",
    "\n",
    "Replicate the tokenization and conversion to input IDs for the following input sentences:\n",
    "1. \"HuggingFace provides amazing NLP resources.\",\n",
    "2. \"I love learning about natural language processing!\",\n",
    "\n",
    "\n",
    "Using the appropriate tokenizer, tokenize these sentences and convert them into input IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "Decoding\n",
    "\n",
    "Convert the vocabulary indices gotten from above back into a human-readable text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 8.\n",
    "\n",
    "What are the tokenized representations of the following sequences: \n",
    "1. 'The mountains are calling and I must go.'\n",
    "2. 'Nature always wears the colors of the spirit.' \n",
    "These sequences are tokenized using the 'distilbert-base-uncased-finetuned-sst-2-english' checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "\n",
    "What are the different tokenization and padding methods applied to the sequences: \n",
    "\n",
    "1. 'The road to success is always under construction.'\n",
    "2. 'A journey of a thousand miles begins with a single step.'\n",
    "\n",
    "These sequences are tokenized using the 'distilbert-base-uncased-finetuned-sst-2-english' checkpoint. The methods employed include padding to the longest sequence, padding to the model's maximum sequence length, padding to a specified maximum length (10), truncation to the model's maximum sequence length, truncation to a specified maximum length (10), and returning the tokenized sequences as PyTorch tensors and NumPy arrays with padding enabled.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
