{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv9YxyHTmQjl"
      },
      "source": [
        "# **Natural Language Processing**\n",
        "\n",
        "Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. Its primary goal is to enable computers to understand, interpret, and generate human language in a valuable way. NLP encompasses a wide range of tasks and applications, including but not limited to:\n",
        "\n",
        "1. **Text Analysis**: NLP is used to analyze and extract information from text data. This can include sentiment analysis, entity recognition, keyword extraction, and more.\n",
        "\n",
        "2. **Machine Translation**: NLP plays a crucial role in machine translation systems like Google Translate, enabling computers to translate text from one language to another.\n",
        "\n",
        "3. **Speech Recognition**: NLP is used in speech recognition systems to transcribe spoken language into text. Virtual assistants like Siri and Alexa rely on NLP for understanding and responding to voice commands.\n",
        "\n",
        "4. **Text Generation**: NLP models can generate human-like text, which has applications in chatbots, content generation, and more. GPT-3 and GPT-4 are examples of powerful text generation models.\n",
        "\n",
        "5. **Question Answering**: NLP can be used to build systems that answer questions based on a given text or knowledge base. These systems are valuable for information retrieval and customer support.\n",
        "\n",
        "6. **Sentiment Analysis**: NLP can determine the sentiment or emotional tone of a piece of text, which is used in applications like social media monitoring and customer feedback analysis.\n",
        "\n",
        "7. **Text Classification**: NLP models can classify text into categories, which is useful for spam detection, topic categorization, and more.\n",
        "\n",
        "8. **Language Understanding**: NLP helps computers understand the nuances of human language, including idioms, sarcasm, and context, making it essential for natural and fluid interactions with users.\n",
        "\n",
        "9. **Named Entity Recognition (NER)**: NER is the process of identifying and classifying named entities such as names of people, organizations, locations, and more in text.\n",
        "\n",
        "10. **Information Extraction**: This involves extracting structured information from unstructured text, such as converting job postings into structured data about job requirements and responsibilities.\n",
        "\n",
        "NLP relies on various techniques and tools, including machine learning, deep learning, and linguistic analysis. Common frameworks and libraries for NLP tasks include NLTK, spaCy, and Hugging Face's Transformers. Many recent advancements in NLP have been driven by large pre-trained language models like BERT, GPT, and others, which have achieved remarkable results in various NLP applications.\n",
        "\n",
        "NLP is a rapidly evolving field with numerous real-world applications, such as chatbots, language translation services, voice assistants, and text analytics for businesses. It continues to be an area of active research and development with the potential to transform the way we interact with computers and process human language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhNeNl6MnRXl"
      },
      "source": [
        "## **Why is it challenging?**\n",
        "\n",
        "Natural Language Processing (NLP) is challenging for several reasons, mainly due to the complexity and ambiguity inherent in human language. Here are some of the key challenges in NLP:\n",
        "\n",
        "1. **Ambiguity**: Language is inherently ambiguous. Words and phrases can have multiple meanings depending on context. For example, the word \"bank\" can refer to a financial institution or the side of a river. Understanding context is a significant challenge in NLP.\n",
        "\n",
        "2. **Syntax and Semantics**: Parsing the syntax and semantics of a sentence accurately is a non-trivial task. Understanding the grammatical structure and the meaning of a sentence requires intricate language models and algorithms.\n",
        "\n",
        "3. **Variability**: Languages are highly variable in terms of dialects, accents, idioms, and colloquialisms. NLP systems must handle this variability to be effective across different populations and regions.\n",
        "\n",
        "4. **Coreference Resolution**: Resolving references, like pronouns, is a complex task. For instance, in the sentence, \"He said he would come,\" understanding which \"he\" refers to whom can be challenging.\n",
        "\n",
        "5. **Anaphora Resolution**: Handling anaphora, where a word or phrase refers back to a previous word or phrase, is a challenging problem. For example, in \"Mary gave birth to a baby. She was very happy,\" resolving \"She\" to \"Mary\" is an anaphora resolution task.\n",
        "\n",
        "6. **Negation and Double Negation**: Understanding negations, double negatives, and their impact on the meaning of a sentence can be challenging. For example, \"I don't dislike pizza\" means \"I like pizza.\"\n",
        "\n",
        "7. **Sarcasm and Irony**: Recognizing sarcasm, irony, and humor in text is challenging because they often rely on context, tone, and cultural knowledge.\n",
        "\n",
        "8. **Lack of Standardization**: Language is not standardized, and people may use different words, phrases, or structures to express the same ideas. NLP models need to be versatile to handle these variations.\n",
        "\n",
        "9. **Data Sparsity**: Training effective NLP models often requires large datasets. However, high-quality labeled data is not always readily available, making it challenging to train accurate models, especially for languages with fewer resources.\n",
        "\n",
        "10. **Multimodal Challenges**: Combining language with other modalities like images or audio introduces additional complexity. Tasks such as image captioning or speech recognition require the fusion of multiple types of data.\n",
        "\n",
        "11. **Bias and Fairness**: NLP models can inherit and even amplify biases present in their training data. Ensuring fairness and mitigating bias is a critical challenge in NLP.\n",
        "\n",
        "12. **Privacy and Security**: NLP can be used to extract sensitive information from text, making privacy and security concerns important. Protecting personal data in text is a challenge.\n",
        "\n",
        "13. **Scalability**: While large pre-trained models have achieved impressive results, they are computationally expensive and may not be easily deployable on all devices or platforms.\n",
        "\n",
        "14. **Domain Adaptation**: NLP models trained on one domain may not perform well in another. Adapting models to specific domains is challenging, as it requires domain-specific data and expertise.\n",
        "\n",
        "15. **Continuous Evolution**: Language is constantly evolving with new words, phrases, and cultural references. NLP systems need to adapt to these changes to remain relevant.\n",
        "\n",
        "Addressing these challenges in NLP requires ongoing research, innovation, and the development of more sophisticated algorithms and models. Researchers are continually working on improving the robustness, accuracy, and real-world applicability of NLP systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVwrmS6upj2K"
      },
      "source": [
        "## **Transformers, what can they do?**\n",
        "\n",
        "Transformers are a type of deep learning model architecture that has had a significant impact on various natural language processing (NLP) and machine learning tasks. Originally introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017, Transformers have since become the foundation for a wide range of applications and have demonstrated remarkable capabilities. Here's what Transformers can do:\n",
        "\n",
        "1. **Sequence-to-Sequence Tasks**: Transformers can perform a wide array of sequence-to-sequence tasks, including machine translation, text summarization, and language generation. Models like the Transformer and its variants, including BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), have achieved state-of-the-art results in these areas.\n",
        "\n",
        "2. **Text Classification**: Transformers are excellent at text classification tasks, such as sentiment analysis, spam detection, and topic categorization. They can learn to represent and classify text effectively, often outperforming traditional machine learning models.\n",
        "\n",
        "3. **Named Entity Recognition (NER)**: Transformers can be used for NER tasks, where they identify and classify named entities like people, organizations, and locations in text.\n",
        "\n",
        "4. **Text Generation**: Transformers are capable of generating human-like text. GPT-3 and GPT-4, for example, have been used to create content, write code, and even engage in natural conversations with users.\n",
        "\n",
        "5. **Question Answering**: Transformers can be used in question-answering systems that can extract answers from text or knowledge bases. For instance, models like BERT have been fine-tuned for this purpose.\n",
        "\n",
        "6. **Language Understanding**: Transformers are essential for language understanding tasks, as they can capture the nuances and context of language. This is crucial for chatbots, virtual assistants, and other applications where understanding user input is vital.\n",
        "\n",
        "7. **Image Captioning**: Transformers can be combined with computer vision models to generate textual descriptions or captions for images. This enables applications like automated image tagging and assistive technologies for the visually impaired.\n",
        "\n",
        "8. **Speech Recognition**: Transformers are used in automatic speech recognition (ASR) systems to transcribe spoken language into text. They help improve the accuracy of speech-to-text conversion.\n",
        "\n",
        "9. **Text Summarization**: Transformers can generate concise summaries of long text documents, making it easier to digest large amounts of information.\n",
        "\n",
        "10. **Language Translation**: Transformers are the foundation of many machine translation systems, like Google Translate, that enable the translation of text from one language to another.\n",
        "\n",
        "11. **Chatbots and Virtual Assistants**: Transformers have been employed in developing conversational agents and virtual assistants like Siri, Alexa, and chatbots that can understand and generate human-like text in real-time conversations.\n",
        "\n",
        "12. **Sentiment Analysis**: Transformers are widely used for sentiment analysis tasks, helping determine the emotional tone of a piece of text, such as whether a review is positive or negative.\n",
        "\n",
        "13. **Recommendation Systems**: Transformers can be used to build recommendation systems by processing user interactions and content to provide personalized recommendations, as seen in platforms like Netflix and Amazon.\n",
        "\n",
        "14. **Language Understanding and Generation Across Languages**: Transformers can be fine-tuned for multiple languages and support multilingual applications, making them versatile for global use.\n",
        "\n",
        "15. **Document Classification**: Transformers are employed in document categorization tasks, such as classifying articles, legal documents, or research papers into specific categories.\n",
        "\n",
        "Transformers have become the backbone of many NLP applications and have demonstrated the ability to understand and generate text in a human-like manner. Their pre-trained models can be fine-tuned for specific tasks, reducing the need for extensive labeled data and making them highly adaptable to a wide range of applications across various domains. They continue to be a driving force in the advancement of NLP and machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Working with pipelines in natural language processing (NLP) typically involves using predefined sequences of NLP tasks or components to process and analyze text data efficiently. Pipelines simplify the development process by automating many of the common tasks. Below is a brief overview of how to work with pipelines in NLP:\n",
        "\n",
        "1. **Select an NLP Library or Framework**: Choose an NLP library or framework that provides pipeline capabilities. Some popular choices include spaCy, Hugging Face Transformers, NLTK, and Gensim.\n",
        "\n",
        "2. **Define the Pipeline**: Create a pipeline by specifying the sequence of NLP tasks you want to perform on your text data. Common tasks in a pipeline may include tokenization, part-of-speech tagging, named entity recognition, sentiment analysis, and more.\n",
        "\n",
        "3. **Load or Preprocess Data**: Prepare your text data by loading it into the NLP framework or library. This may involve reading text from files, databases, or web sources and performing any necessary preprocessing steps, such as cleaning, lowercasing, or encoding.\n",
        "\n",
        "4. **Instantiate the Pipeline**: In your chosen NLP library, instantiate a pipeline object and configure it with the tasks and components you defined in step 2. This sets up the sequence of NLP operations to be executed.\n",
        "\n",
        "5. **Process Data**: Apply the pipeline to your text data. This will automatically run the predefined NLP tasks on your text, producing the desired output. The pipeline takes care of passing data between tasks and handling intermediate results.\n",
        "\n",
        "6. **Access Results**: After processing your text data with the pipeline, you can access the results of each task. These results may include tokenized text, part-of-speech tags, named entities, sentiment scores, or any other information generated by the pipeline components.\n",
        "\n",
        "7. **Customization**: Some NLP libraries allow you to customize or extend the pipeline by adding or replacing components to tailor the processing to your specific needs. You can add custom functions or components for tasks like domain-specific entity recognition.\n",
        "\n",
        "8. **Post-Processing**: Depending on your application, you may need to perform additional post-processing or analysis on the pipeline's output. This can include aggregating information, generating reports, or integrating the results into other applications.\n",
        "\n",
        "9. **Evaluation and Fine-Tuning**: Evaluate the pipeline's performance on your specific tasks or datasets. If necessary, fine-tune the pipeline by adjusting configurations, component choices, or training on custom data.\n",
        "\n",
        "Here's an example of working with a spaCy pipeline for basic text processing:\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "# Load the English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a text for processing\n",
        "text = \"This is an example sentence for NLP pipeline demonstration.\"\n",
        "\n",
        "# Process the text with the spaCy pipeline\n",
        "doc = nlp(text)\n",
        "\n",
        "# Access results, e.g., tokenized text, part-of-speech tags, named entities, etc.\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.ent_type_)\n",
        "\n",
        "# Perform further analysis or tasks with the processed text data\n",
        "```\n",
        "\n",
        "Using pipelines in NLP can save you time and effort by automating common text processing tasks and allowing you to focus on specific analysis or application-related tasks. Depending on the NLP library or framework you choose, the specific steps and capabilities may vary, but the general process is similar. For the rest of the course, we will be using Hugging Face Transformer Pipeline "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxz4EGFtqU8i"
      },
      "source": [
        "The most basic object in the Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXt_CgUBiU7T"
      },
      "source": [
        "## SETUP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnUluOHchcm6",
        "outputId": "61f8a69c-c062-44a7-f3f3-8e0da11997df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/python/3.10.8/lib/python3.10/site-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (1.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Installing a light version of transformers\n",
        "\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRSjwSxYjPHX"
      },
      "source": [
        " Since we’ll be using a lot of different features of the library, we recommend installing the development version, which comes with all the required dependencies for pretty much any imaginable use case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScJ8lcj2jOXm",
        "outputId": "54bbcb9c-21b0-4eb5-887d-db5f08ec4e96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/python/3.10.8/lib/python3.10/site-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.10/site-packages (from transformers[sentencepiece]) (1.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers[sentencepiece]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers[sentencepiece]) (2023.10.3)\n",
            "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers[sentencepiece]) (4.66.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/python/3.10.8/lib/python3.10/site-packages (from transformers[sentencepiece]) (4.25.0)\n",
            "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (2023.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "03ac5c5626c9471594442aa1e87c07fe",
            "99816fb479da4bad90780f5f44889dbd",
            "677b1187ee314498b4f0f5b090601b7a",
            "eaae09be1ecf404393e1a503575e6326",
            "218181680b8c43b6a561c3498f3ab557",
            "4ad15a761825490d982e71ca2df0dff1",
            "dc87d82eb7e94989ae462328229a1b83",
            "8eeb57da42cd43efa4ff2eaed0b83355",
            "da3a5cf5d12c4e4794150101f05178d1",
            "4822d15150ac4cb9b822b626d12093ce",
            "1f56454e07334007bca84f7a957b856f",
            "c86f3c1b7c64407abadf78ad3b1056f5",
            "44f9fb468f974cafb4f50253b749c042",
            "aa95a903390343cba6653c90da982bf6",
            "c5705bd1c82f401cb9675c0d4cb94045",
            "c7e9948dbed14620973c9cc5cf2a265c",
            "7dad16a8c4064fe6890dd93b9f7b6db1",
            "87906cf526d84717ab9d8e6547336257",
            "21178e9225de463494fc2d73b58f2c1a",
            "13bcac74bcbb46219996078a3daeaf36",
            "eaa02bf7dd7a4010af43dd95c83e2b76",
            "e65bb27895e94077a28cea035dbd399e",
            "84776f27dc8b429db6fa186ce1e7661b",
            "35a06f5db3ef4972a7abd1897e7fb5f2",
            "c2d2da4588e64889a3db2aff47e2be20",
            "de9bd01014a94418a64ba813c468e524",
            "a20b001d5f0446f7a7be7f6506ef7d88",
            "f69354c92faa4e5bb4a7d1b6ca6071cd",
            "7a49585efaea4cf3ba9797ab5eebb5e5",
            "c82717ff51954d049f545421b7ada1c9",
            "9ce146470f474a429af294d52563a22c",
            "5af149d50e2648479aab67ec2dd5ff32",
            "797b4a33d9d74282a110b8b4b3e323fe",
            "2e579ee522fe4e479433f106b51f21d5",
            "5daa8ad3c3f54445a7a1af0b024894e7",
            "d69c9cacdc5f4b1583826f2099cd00d9",
            "2fe1d42694eb4d26b6194d17da0cc187",
            "001a44e0b7bd4bb296ac1136869ed490",
            "a967c9408e814554a0953b7a1ba25c06",
            "96738277fda147268994e8224da1d623",
            "f101fee70cc746088069b18f9246e438",
            "70a8e863503d4f008edf382a63d4acf3",
            "2fa580965e5d47d9a248f1fa1fdee32e",
            "52394f90dd9f475786ec753deb975f89"
          ]
        },
        "id": "F8Hu450UqfCf",
        "outputId": "5de2749f-8343-4ba4-d2d0-5dce5a06d09a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9997721314430237}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"Gosh! I am so tired\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp6sI6ukv53i"
      },
      "source": [
        "Passing Multiple sentence: To Pass multiple sentence, you can put it in a list seperated by commas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ORMXXLFv85b",
        "outputId": "59d11443-2b32-4bb6-be6e-89d9befa791b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9940189123153687},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier(\n",
        "    [\"I've been waiting for this course from Hamoye, it's finally here.\", \"I hate this so much!\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFENpPT4v5pp"
      },
      "source": [
        "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
        "\n",
        "There are three main steps involved when you pass some text to a pipeline:\n",
        "\n",
        "1. The text is preprocessed into a format the model can understand.\n",
        "2. The preprocessed inputs are passed to the model.\n",
        "3. The predictions of the model are post-processed, so you can make sense of them.\n",
        "\n",
        "Some of the currently available pipelines are:\n",
        "\n",
        "* feature-extraction (get the vector representation of a text)\n",
        "* fill-mask\n",
        "* ner (named entity recognition)\n",
        "* question-answering\n",
        "* sentiment-analysis\n",
        "* summarization\n",
        "* text-generation\n",
        "* translation\n",
        "* zero-shot-classification\n",
        "\n",
        "Let’s have a look at a few of these!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0dru_n5x65S"
      },
      "source": [
        "### **Zero-shot classification**\n",
        "\n",
        "\"Zero-shot classification\" refers to a machine learning or deep learning approach where a model is trained to classify objects or data into categories it has never seen during training. This concept is particularly important in the context of natural language processing and computer vision.\n",
        "\n",
        "A brief overview of zero-shot classification:\n",
        "\n",
        "1. **Traditional Classification**: In traditional classification tasks, a machine learning model is trained on labeled data with predefined categories. For instance, in text classification, a model might be trained on a dataset with various topics like \"sports,\" \"politics,\" and \"technology.\"\n",
        "\n",
        "2. **Zero-Shot Classification**: In zero-shot classification, the model is expected to classify data into categories that were not part of its training data. This means that the model must generalize its knowledge to recognize and classify new, unseen categories accurately.\n",
        "\n",
        "3. **Semantic Understanding**: Zero-shot classification often relies on semantic understanding of the data. For example, in natural language processing, models may be trained to understand the meaning of words or phrases, which allows them to categorize text into unseen categories based on their semantic similarity to known categories.\n",
        "\n",
        "4. **Attributes and Embeddings**: In zero-shot classification, models may use attributes or embeddings to represent categories and data points. These embeddings capture the essence of categories and data in a continuous space, allowing the model to reason about similarities and differences between them.\n",
        "\n",
        "5. **Example Use Cases**:\n",
        "   - In text classification, a model trained on articles about animals could be asked to classify text about \"marsupials,\" a category it has never seen during training.\n",
        "   - In computer vision, an object recognition model might be tasked with identifying a \"Segway\" even if it was not part of its training data.\n",
        "\n",
        "6. **Challenges**:\n",
        "   - Zero-shot classification can be challenging, as the model must make inferences about categories it has no direct knowledge of.\n",
        "   - Ensuring the model's generalization is accurate and that it can handle a wide range of unseen categories is a complex task.\n",
        "\n",
        "7. **Approaches**:\n",
        "   - Zero-shot learning often involves techniques like attribute-based classification, where models are trained to understand category attributes and reason about new categories based on their attributes.\n",
        "   - Pre-trained language models and embeddings (e.g., Word2Vec, GloVe) have been used in zero-shot classification to leverage semantic information.\n",
        "\n",
        "In summary, zero-shot classification is a fascinating area of machine learning that focuses on extending the capabilities of models to classify data into categories they have never seen. It's particularly valuable in cases where new categories emerge or where the model needs to adapt to a dynamic and evolving environment. It requires a deep understanding of semantics and the ability to generalize from known data to unknown categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'sequence': 'This is a course about the Transformers library',\n",
              " 'labels': ['education', 'business', 'politics'],\n",
              " 'scores': [0.8445998430252075, 0.11197364330291748, 0.04342653974890709]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"This is a course about the Transformers library\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKkUfP3kx6dj"
      },
      "source": [
        "This pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want! Play around with your own sequences and labels and see how the model behaves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Text Generation**\n",
        "\n",
        "Text generation is a natural language processing (NLP) task where a model generates human-like text based on a given input or prompt. It involves teaching a machine learning model to understand the structure and context of language and use that understanding to create coherent and contextually relevant text. Here's a brief overview:\n",
        "\n",
        "1. **Objective**: The goal of text generation is to produce textual content that appears as if it were written by a human. It's used in applications like chatbots, content generation, creative writing, code generation, and more.\n",
        "\n",
        "2. **Input Types**: Text generation models can take various types of input, such as a single word, a sentence, or a paragraph. The input serves as a starting point or a prompt for generating text.\n",
        "\n",
        "3. **Techniques**: Various techniques are employed in text generation, including recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and more recently, Transformers. These models learn to predict the next word or sequence of words based on the context provided.\n",
        "\n",
        "4. **Pre-trained Models**: Many text generation tasks benefit from pre-trained language models like GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers). These models can be fine-tuned for specific text generation tasks.\n",
        "\n",
        "5. **Applications**:\n",
        "   - **Chatbots**: Text generation is used to enable chatbots to provide human-like responses in real-time conversations.\n",
        "   - **Content Generation**: It's employed for generating articles, product descriptions, and marketing content automatically.\n",
        "   - **Language Translation**: In machine translation, it can be used to generate translations from one language to another.\n",
        "   - **Code Generation**: Text generation models can generate code snippets or scripts based on user requirements.\n",
        "   - **Storytelling and Creative Writing**: They can assist authors and creative writers in generating content or ideas.\n",
        "\n",
        "6. **Challenges**: Text generation faces challenges related to coherence, relevance, and avoiding bias. Ensuring that generated text is contextually appropriate and free from unintended biases is an ongoing challenge.\n",
        "\n",
        "7. **Use of Prompts**: Prompting is a crucial aspect of text generation. The quality of the prompt often influences the quality and relevance of the generated text.\n",
        "\n",
        "8. **Fine-tuning**: Many text generation models are fine-tuned on specific tasks or datasets to make them more effective for a particular application.\n",
        "\n",
        "In summary, text generation is a versatile NLP task used to create human-like text for various applications. It leverages a deep understanding of language structure and context and has seen significant advancements with the emergence of Transformer-based models. These models have opened up new possibilities for automated content creation and natural language interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HsllyeRdx3gj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to build high quality, accurate digital cameras online and the how of working with your camera in live footage.\\n\\nWe will build a DSLR that shoots better video than those other high end models we have'}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this course, we will teach you how to\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using a model the pipeline\n",
        "The previous examples used the default model for the task at hand. You can control how many different sequences are generated with the argument num_return_sequences and the total length of the output text with the argument max_length.\n",
        "\n",
        "Let’s try the distilgpt2 model! Here’s how to load it in the same pipeline as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fPvmS07Ywmff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to master the most basic mechanics in a game with no idea what is at stake.”\\n\\nThis course provides you an instruction set that covers the'},\n",
              " {'generated_text': 'In this course, we will teach you how to learn about the process of selecting your own image for the screen.'}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=40,\n",
        "    num_return_sequences=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Mask Filling**\n",
        "\n",
        "Mask filling, also known as cloze tasks or masked language modeling, is a type of natural language processing (NLP) task where a model is presented with a sentence or text with certain words or tokens replaced by a special \"mask\" token (often represented as \"[MASK]\"). The model's objective is to predict the missing words or tokens that were replaced with the mask. This task is closely associated with pre-trained language models like BERT (Bidirectional Encoder Representations from Transformers) and its variants. Here's a brief overview:\n",
        "\n",
        "1. **Objective**: The main goal of mask filling is to assess a model's understanding of the context and semantics of a sentence. By predicting the masked words or tokens, the model demonstrates its ability to comprehend the surrounding text and generate contextually relevant replacements.\n",
        "\n",
        "2. **Use in Pre-trained Models**: Many pre-trained language models, such as BERT, are trained using mask filling as one of their core tasks. During pre-training, a large corpus of text is used to mask random words in sentences, and the model learns to predict those masked words based on the surrounding context.\n",
        "\n",
        "3. **Examples**:\n",
        "   - Input Sentence: \"The quick brown [MASK] jumped over the lazy dog.\"\n",
        "   - Expected Output: \"The quick brown fox jumped over the lazy dog.\"\n",
        "\n",
        "   In this example, the goal is to predict that the masked word is \"fox.\"\n",
        "\n",
        "   - Input Sentence: \"[MASK] is the capital of France.\"\n",
        "   - Expected Output: \"Paris is the capital of France.\"\n",
        "\n",
        "   Here, the model should predict \"Paris\" as the masked word.\n",
        "\n",
        "4. **Evaluation**: Mask filling tasks are often used to evaluate the contextual understanding and language proficiency of NLP models. The models are evaluated based on how accurately they predict the missing words.\n",
        "\n",
        "5. **Applications**:\n",
        "   - Mask filling can be used to assess a model's general language understanding and its ability to fill in missing information, making it a valuable tool for language understanding tasks.\n",
        "   - It's used in fine-tuning pre-trained models for specific NLP tasks, such as text classification, text generation, and question answering, to improve their contextual understanding.\n",
        "\n",
        "6. **Pre-training and Fine-tuning**: Pre-trained models that have been trained using mask filling can be fine-tuned on specific downstream tasks by adding additional output layers. This fine-tuning process leverages the contextual understanding gained during pre-training to excel in a wide range of NLP applications.\n",
        "\n",
        "In summary, mask filling is a fundamental NLP task that helps models develop a deep understanding of language context. It plays a key role in the development of pre-trained language models like BERT and is widely used in NLP research and applications where contextually accurate predictions are crucial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6229b9bebf14840b6bd78fd61a68926",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "703e5fe4e5764223995d7960fb53d014",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e413a90552ce446897542e0e9205e5cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30cbe72b988e4ddba999145c75cb0cd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bb4e679d10f4f5bb87e7da7f22ff194",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 0.1961979866027832,\n",
              "  'token': 30412,\n",
              "  'token_str': ' mathematical',\n",
              "  'sequence': 'This course will teach you all about mathematical models.'},\n",
              " {'score': 0.04052741825580597,\n",
              "  'token': 38163,\n",
              "  'token_str': ' computational',\n",
              "  'sequence': 'This course will teach you all about computational models.'}]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The top_k argument controls how many possibilities you want to be displayed. Note that here the model fills in the special <mask> word, which is often referred to as a mask token. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Named Entity Recognition (NER)**\n",
        "\n",
        "Named Entity Recognition (NER) is a natural language processing (NLP) task that focuses on identifying and classifying named entities in text. Named entities are specific words or phrases that refer to real-world objects, such as names of people, organizations, locations, dates, percentages, and more. NER is essential for extracting structured information from unstructured text. Here's a brief overview:\n",
        "\n",
        "1. **Objective**: The primary goal of Named Entity Recognition is to locate and classify named entities within a text. It helps transform unstructured text into structured data, making it more useful for various NLP applications.\n",
        "\n",
        "2. **Named Entity Types**: NER typically involves classifying entities into several predefined categories, such as:\n",
        "   - Person: Names of individuals, like \"John Smith.\"\n",
        "   - Organization: Names of companies, institutions, or groups, e.g., \"Google\" or \"UNICEF.\"\n",
        "   - Location: Geographical names, like \"New York\" or \"Mount Everest.\"\n",
        "   - Date: Expressions of time and dates, such as \"July 4, 1776.\"\n",
        "   - Time: Specific times or time intervals, e.g., \"3:00 PM\" or \"two hours.\"\n",
        "   - Percentage: Percentage values, like \"50%\" or \"75 percent.\"\n",
        "\n",
        "3. **Applications**:\n",
        "   - Information Extraction: NER is used in applications that require extracting specific information from text, such as news articles or business documents.\n",
        "   - Chatbots and Virtual Assistants: NER helps chatbots understand user queries and extract relevant entities for generating responses.\n",
        "   - Document Categorization: NER can assist in categorizing documents or articles by identifying entities.\n",
        "   - Speech Recognition: In transcription services, NER is used to identify and label named entities in spoken language.\n",
        "   - Search Engines: NER enhances search engines by recognizing named entities in search queries and documents.\n",
        "\n",
        "4. **Challenges**:\n",
        "   - Ambiguity: Words may have different meanings depending on context. For example, \"Apple\" could refer to the company or the fruit.\n",
        "   - Out-of-Vocabulary Entities: NER systems need to handle new, previously unseen named entities.\n",
        "   - Variations: People and organizations may have multiple names or aliases.\n",
        "   - Multilingual Support: NER must work across various languages and adapt to linguistic differences.\n",
        "\n",
        "5. **Techniques**: NER can be performed using various techniques, including rule-based approaches, statistical models, and machine learning methods. Deep learning models, such as BiLSTM-CRF and Transformers, have shown great success in NER tasks, especially when trained on large datasets.\n",
        "\n",
        "6. **Evaluation**: NER systems are evaluated using metrics like precision, recall, and F1 score, which measure the system's ability to correctly identify and classify named entities in text.\n",
        "\n",
        "In summary, Named Entity Recognition is a vital NLP task for identifying and categorizing named entities within text, enabling structured data extraction and supporting a wide range of applications that rely on understanding and processing unstructured text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.9871352,\n",
              "  'word': 'Goodrich',\n",
              "  'start': 11,\n",
              "  'end': 19},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': 0.9463431,\n",
              "  'word': 'Hamoye',\n",
              "  'start': 34,\n",
              "  'end': 40},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9997075,\n",
              "  'word': 'Bermuda',\n",
              "  'start': 44,\n",
              "  'end': 51}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"My name is Goodrich and I work at Hamoye in Bermuda.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Question Answering (QA)**\n",
        "\n",
        "Question Answering (QA) is a natural language processing (NLP) task in which a computer system is designed to answer questions posed in natural language, often based on a given context or a specific knowledge base. QA systems are widely used for information retrieval, customer support, and a variety of applications. Here's a brief overview:\n",
        "\n",
        "1. **Objective**: The primary goal of Question Answering is to provide a precise and contextually relevant answer to a user's question. This can involve either open-domain QA, where the system retrieves information from a vast knowledge base, or closed-domain QA, where the system answers questions based on a specific, predefined set of documents or data.\n",
        "\n",
        "2. **Components**: QA systems typically consist of two main components:\n",
        "   - **Question Understanding**: This component involves parsing and understanding the user's question, including identifying key entities and their relationships.\n",
        "   - **Answer Generation**: The system searches for the relevant information and formulates a concise and informative answer.\n",
        "\n",
        "3. **Types of QA**:\n",
        "   - **Factoid QA**: In this type, the user asks for specific factual information, such as \"What is the capital of France?\" The answer is usually a single entity or fact.\n",
        "   - **Opinion QA**: Users seek opinions or subjective information, like \"What is the best restaurant in town?\" The answers are often subjective and based on reviews or recommendations.\n",
        "   - **List QA**: Users request lists of information, such as \"List the planets in our solar system.\" The answers may include multiple entities.\n",
        "   - **Mathematical QA**: This type involves solving mathematical problems or equations, like \"What is 5 multiplied by 12?\"\n",
        "\n",
        "4. **Applications**:\n",
        "   - **Information Retrieval**: QA systems can be used to extract specific information from a large corpus of documents or the internet.\n",
        "   - **Virtual Assistants**: Voice-activated virtual assistants like Siri and Alexa use QA to answer user queries.\n",
        "   - **Customer Support**: QA chatbots can assist customers by answering frequently asked questions and providing support.\n",
        "   - **Search Engines**: QA techniques are used in search engines to provide direct answers to user queries.\n",
        "   - **Education**: QA systems can be used in educational applications, helping students find answers to their questions in textbooks or online resources.\n",
        "\n",
        "5. **Challenges**:\n",
        "   - **Ambiguity**: Many questions can have multiple valid interpretations, making it challenging to provide accurate answers.\n",
        "   - **Context Understanding**: Understanding context is crucial, as answers often depend on the context provided in the question or surrounding text.\n",
        "   - **Scalability**: Open-domain QA systems must be capable of searching and processing vast amounts of data efficiently.\n",
        "   - **Multimodal QA**: Handling questions that involve both text and other modalities like images or audio adds complexity to QA systems.\n",
        "\n",
        "6. **Techniques**: QA systems utilize a range of techniques, including rule-based approaches, information retrieval methods, machine learning models (such as BERT or GPT-based models), and reinforcement learning for complex question answering.\n",
        "\n",
        "In summary, Question Answering is a fundamental NLP task that aims to enable machines to understand and answer questions in natural language, making it a valuable tool for information retrieval, customer support, and various applications where interaction with unstructured data is essential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.5029252767562866,\n",
              " 'start': 34,\n",
              " 'end': 51,\n",
              " 'answer': 'Hamoye in Bermuda'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Goodrich and I work at Hamoye in Bermuda.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Text Summarization**\n",
        "\n",
        "Text Summarization is a natural language processing (NLP) task that involves generating a concise and coherent summary of a longer document or a piece of text while retaining the most important information and the overall meaning. Here's a brief overview:\n",
        "\n",
        "1. **Objective**: The primary goal of text summarization is to condense a larger body of text into a shorter version that captures the key ideas, main points, and essential information, making it more accessible and easier to comprehend.\n",
        "\n",
        "2. **Types of Summarization**:\n",
        "   - **Extractive Summarization**: In extractive summarization, the summary is generated by selecting and extracting sentences or phrases directly from the original text. These selected segments are considered representative of the main content.\n",
        "   - **Abstractive Summarization**: Abstractive summarization involves generating a summary by paraphrasing and rephrasing the content, potentially using different words and sentence structures. It aims to provide a more human-like summary and is generally more challenging.\n",
        "\n",
        "3. **Applications**:\n",
        "   - **News Summarization**: Automatically creating concise news articles or briefs from longer news reports.\n",
        "   - **Document Summarization**: Summarizing long documents, research papers, or legal documents for quick understanding.\n",
        "   - **Search Engines**: Search engines often provide snippets of summarized content in search results.\n",
        "   - **Content Generation**: Generating short descriptions or previews for content recommendations, such as movie summaries or product descriptions.\n",
        "   - **Document Clustering and Organization**: Summarization can help group similar documents together by creating summaries that represent document clusters.\n",
        "\n",
        "4. **Challenges**:\n",
        "   - **Content Selection**: In extractive summarization, choosing the most relevant sentences or phrases is a non-trivial task.\n",
        "   - **Fluency and Coherence**: Abstractive summarization requires generating summaries that are fluent, coherent, and contextually accurate.\n",
        "   - **Preserving Core Information**: Summaries must retain essential information while eliminating redundancy and non-essential details.\n",
        "   - **Multimodal Summarization**: Handling text and other modalities like images or audio can be challenging in multimodal summarization.\n",
        "\n",
        "5. **Techniques**: Text summarization can be performed using various techniques, including rule-based methods, statistical models, and machine learning approaches. Deep learning models, such as transformers, have shown significant advancements in abstractive summarization tasks.\n",
        "\n",
        "6. **Evaluation**: Summarization systems are evaluated using metrics like ROUGE (Recall-Oriented Understudy for Gisting Evaluation) and BLEU (Bilingual Evaluation Understudy). These metrics measure the quality of summaries based on their overlap with reference summaries or human-generated summaries.\n",
        "\n",
        "In summary, text summarization is a valuable NLP task that aims to make large volumes of information more accessible by generating concise and coherent summaries. It has numerous applications in news media, research, content recommendation, and more, and it plays a crucial role in making complex information more digestible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of \n",
        "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
        "    the premier American universities engineering curricula now concentrate on \n",
        "    and encourage largely the study of engineering science. As a result, there \n",
        "    are declining offerings in engineering subjects dealing with infrastructure, \n",
        "    the environment, and related issues, and greater concentration on high \n",
        "    technology subjects, largely supporting increasingly complex scientific \n",
        "    developments. While the latter is important, it should not be at the expense \n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other \n",
        "    industrial countries in Europe and Asia, continue to encourage and advance \n",
        "    the teaching of engineering. Both China and India, respectively, graduate \n",
        "    six and eight times as many traditional engineers as does the United States. \n",
        "    Other industrial countries at minimum maintain their output, while America \n",
        "    suffers an increasingly serious decline in the number of engineering graduates \n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Translation**\n",
        "\n",
        "Translation, in the context of natural language processing (NLP), is the task of converting text or speech from one language to another while preserving the meaning and context. It involves understanding the source language and generating a corresponding text in the target language. Here's a brief overview:\n",
        "\n",
        "1. **Objective**: The primary goal of translation is to facilitate communication between people who speak different languages by providing them with a readable and coherent text or speech in their preferred language.\n",
        "\n",
        "2. **Types of Translation**:\n",
        "   - **Machine Translation**: Machine translation involves the use of computer programs and algorithms to automatically translate text or speech from one language to another. It can be further categorized into:\n",
        "     - **Statistical Machine Translation (SMT)**: This approach relies on statistical models to translate text and has been widely used in the past.\n",
        "     - **Neural Machine Translation (NMT)**: NMT utilizes deep learning models, such as neural networks, to improve translation quality, making it the dominant method in recent years.\n",
        "   - **Human Translation**: Human translation is performed by human translators who are proficient in both the source and target languages. It is often used for high-quality and contextually sensitive translations, such as legal or literary works.\n",
        "\n",
        "3. **Applications**:\n",
        "   - **Global Communication**: Translation enables people around the world to communicate, share information, and access content in different languages.\n",
        "   - **Content Localization**: Businesses and organizations use translation to adapt their content, products, and services for specific target markets and audiences.\n",
        "   - **Website Translation**: Websites and online platforms translate content to reach a broader international audience.\n",
        "   - **Literary Translation**: Translators convert books, poetry, and other literary works into various languages to make them accessible to a global readership.\n",
        "   - **Machine Translation Tools**: Translation tools and services like Google Translate provide quick and automated translation for a wide range of content.\n",
        "\n",
        "4. **Challenges**:\n",
        "   - **Context and Nuance**: Translating idiomatic expressions and cultural nuances accurately can be challenging.\n",
        "   - **Ambiguity**: Languages often contain words or phrases with multiple meanings, and determining the correct translation depends on context.\n",
        "   - **Language Specifics**: Some languages have unique linguistic features that do not directly translate into other languages.\n",
        "   - **Machine Translation Quality**: Machine translation systems may produce errors or unnatural-sounding translations, especially for less common languages.\n",
        "\n",
        "5. **Techniques**: Machine translation relies on a range of techniques, including phrase-based translation, attention mechanisms, and neural network architectures like transformers (e.g., in models like Google's BERT or OpenAI's GPT-3). These models are trained on large multilingual datasets and can provide high-quality translations.\n",
        "\n",
        "6. **Evaluation**: The quality of translation systems is evaluated using metrics such as BLEU (Bilingual Evaluation Understudy) and METEOR, which measure the similarity and fluency of machine-generated translations compared to human references.\n",
        "\n",
        "In summary, translation plays a pivotal role in breaking down language barriers and enabling global communication and understanding. It is used in a wide range of applications, from business and technology to literature and cultural exchange. Advances in NLP and machine translation have significantly improved the quality and accessibility of translation services in recent years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **How do Transformer work?**\n",
        "\n",
        "Transformers are a type of deep learning model architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. They have since become a cornerstone in various natural language processing (NLP) and machine learning tasks. Transformers work through a mechanism called the \"self-attention mechanism\" and consist of several key components:\n",
        "\n",
        "1. **Input Embedding**: Transformers take a sequence of tokens as input. Each token (e.g., word or subword) is converted into a fixed-dimensional vector representation, often called word embeddings or token embeddings. These embeddings capture the semantic meaning of the tokens.\n",
        "\n",
        "2. **Positional Encoding**: Unlike traditional recurrent models, transformers do not have inherent notions of the order of tokens in a sequence. To address this, positional encodings are added to the token embeddings. These positional encodings provide information about the position of tokens within the sequence.\n",
        "\n",
        "3. **Self-Attention Mechanism**: This is the core of the transformer architecture. Self-attention allows each token to consider the relationships and dependencies between all other tokens in the sequence, which is crucial for understanding the context. The self-attention mechanism computes weighted sums of all tokens, where the weights are learned during training and are based on the similarity between tokens.\n",
        "\n",
        "   - The self-attention mechanism operates on a sequence of token embeddings and computes different attention weights for each token based on how it relates to other tokens in the sequence.\n",
        "   - The attention weights are used to generate weighted representations for each token, considering its relationships with all other tokens. This enables the model to focus on relevant context and ignore irrelevant information.\n",
        "\n",
        "4. **Multiple Layers**: Transformers consist of multiple layers, typically referred to as \"transformer blocks\" or \"encoder-decoder layers.\" Each layer consists of a stack of self-attention mechanisms and feedforward neural networks.\n",
        "\n",
        "5. **Encoder-Decoder Architecture** (for tasks like translation): In sequence-to-sequence tasks, such as language translation, transformers use an encoder-decoder architecture. The encoder processes the input sequence, while the decoder generates the output sequence.\n",
        "\n",
        "6. **Masking**: In tasks where it's essential to process sequences with variable lengths (e.g., machine translation), transformers use masking to ensure that each position in the output sequence only depends on positions in the input sequence with valid information.\n",
        "\n",
        "7. **Position-wise Feedforward Networks**: After self-attention, each token representation goes through a position-wise feedforward network. This network applies a set of fully connected layers separately to each position, enhancing the model's capacity to capture complex relationships between tokens.\n",
        "\n",
        "8. **Residual Connections and Layer Normalization**: Transformers use residual connections and layer normalization to facilitate training deeper networks and improve gradient flow.\n",
        "\n",
        "9. **Output Layer**: The final layer in the transformer produces the model's predictions. For sequence-to-sequence tasks, the decoder uses self-attention mechanisms in addition to source-target attention mechanisms to generate the output sequence step by step.\n",
        "\n",
        "10. **Training and Fine-Tuning**: Transformers are typically pre-trained on large corpora of text data using objectives like language modeling or masked language modeling (as seen in models like BERT and GPT). After pre-training, models can be fine-tuned on specific tasks using task-specific labeled data.\n",
        "\n",
        "Transformers have demonstrated remarkable capabilities in various NLP tasks and beyond, including machine translation, text classification, question answering, text generation, and more. Their self-attention mechanism, which allows them to capture complex and long-range dependencies in data, has contributed to their success in understanding and generating natural language text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Encoder Models**\n",
        "\n",
        "Encoder models, in the context of natural language processing (NLP) and machine learning, are a class of deep learning architectures that focus on encoding and understanding input data, particularly in the form of text or sequences. These models are widely used for a range of NLP tasks, including text classification, sentiment analysis, machine translation, question answering, and more. Encoder models are the foundation of many state-of-the-art NLP systems. Here's an overview of how encoder models work and their key features:\n",
        "\n",
        "1. **Input Data Encoding**: Encoder models take input data in the form of sequences, such as text. The input sequence is typically tokenized and embedded into continuous vector representations. Each token in the sequence is transformed into a vector through an embedding layer.\n",
        "\n",
        "2. **Deep Neural Networks**: Encoder models often employ deep neural networks, such as recurrent neural networks (RNNs), long short-term memory networks (LSTMs), or more commonly, transformers, to process the token embeddings. Transformers, in particular, have gained prominence due to their effectiveness and parallel processing capabilities.\n",
        "\n",
        "3. **Sequence Processing**: The primary task of an encoder is to process the input sequence while capturing meaningful contextual information. In the case of transformers, this is achieved through self-attention mechanisms, which allow each token to attend to all other tokens in the sequence, enabling the model to capture dependencies and context effectively.\n",
        "\n",
        "4. **Contextual Representations**: As the model processes the input sequence, it updates the token embeddings to produce contextual representations. These representations encode not only the content of the tokens themselves but also their relationships with other tokens in the sequence.\n",
        "\n",
        "5. **Layer Stacking**: Many encoder models consist of multiple layers, which enables them to capture increasingly abstract and complex patterns in the input data. In the case of transformers, these layers can be stacked to create deep models.\n",
        "\n",
        "6. **Residual Connections and Layer Normalization**: Encoder models often incorporate residual connections and layer normalization between layers to stabilize training and facilitate gradient flow in deep networks.\n",
        "\n",
        "7. **Dimension Reduction**: Encoder models may reduce the dimensionality of the representations in higher layers to focus on the most relevant information and reduce computational complexity.\n",
        "\n",
        "8. **Pre-training and Fine-Tuning**: Many encoder models are pre-trained on large corpora of text data using objectives like language modeling or masked language modeling. After pre-training, they can be fine-tuned on specific downstream tasks using task-specific labeled data.\n",
        "\n",
        "9. **Adaptability**: Encoder models are versatile and can be adapted to various NLP tasks. By fine-tuning the final layers and output, they can be tailored to specific tasks like text classification, sentiment analysis, or named entity recognition.\n",
        "\n",
        "10. **Multimodal Input**: While encoder models are often associated with processing text, they can also be extended to handle multimodal data, combining text with other modalities like images, audio, or structured data.\n",
        "\n",
        "Notable encoder models include BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and various other variants. These models have achieved state-of-the-art performance in a wide range of NLP tasks and continue to advance the field of natural language understanding and processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Decoder Models**\n",
        "\n",
        "Decoder models, in the context of natural language processing (NLP) and machine learning, are a class of deep learning architectures designed for generating sequences of data. Unlike encoder models, which focus on encoding input data, decoder models specialize in producing sequential output, making them well-suited for tasks like language generation, machine translation, text summarization, and more. Here's an overview of how decoder models work and their key features:\n",
        "\n",
        "1. **Input-Encoding and Context**: In many cases, decoder models work in conjunction with encoder models. The encoder processes the input sequence and generates contextual representations, often referred to as the \"context\" or \"thought vector.\" This context encodes relevant information from the input data.\n",
        "\n",
        "2. **Sequential Output**: The primary task of a decoder is to generate a sequence of data. This sequence could be text, translation, summarization, or any other task that requires generating ordered data. For language generation, each step involves producing a token or word one at a time.\n",
        "\n",
        "3. **Deep Neural Networks**: Decoder models typically use deep neural networks to generate the output sequence. These networks are often designed as recurrent neural networks (RNNs), long short-term memory networks (LSTMs), transformers, or other sequence-to-sequence models.\n",
        "\n",
        "4. **Autoregressive Generation**: Many decoder models use autoregressive generation, where the model produces one token at a time while taking into account previously generated tokens. The model's hidden state or context evolves with each generated token, and the generated token is used as input for the next step.\n",
        "\n",
        "5. **Layer Stacking**: Decoder models can consist of multiple layers, allowing them to capture increasingly complex patterns in the output sequence and enhance the quality of the generated content.\n",
        "\n",
        "6. **Attention Mechanisms**: Attention mechanisms are commonly used in decoder models to focus on different parts of the input context or previously generated tokens when generating the current token. This enables the model to capture dependencies and context effectively.\n",
        "\n",
        "7. **Residual Connections and Layer Normalization**: Similar to encoder models, decoder models may incorporate residual connections and layer normalization between layers to improve training stability and facilitate gradient flow in deep networks.\n",
        "\n",
        "8. **Dimension Reduction**: Some decoder models reduce the dimensionality of the hidden representations in higher layers to focus on the most relevant information and reduce computational complexity.\n",
        "\n",
        "9. **Training and Fine-Tuning**: Decoder models are often trained with supervised learning, where they are provided with input data and target sequences to generate. After pre-training, they can be fine-tuned on specific tasks with task-specific labeled data.\n",
        "\n",
        "10. **Conditional Generation**: Decoder models can perform conditional generation, taking additional context or conditioning information to influence the output sequence. For example, in machine translation, the decoder takes the source language context as condition to generate the target language translation.\n",
        "\n",
        "Notable decoder models include models like the GPT (Generative Pre-trained Transformer) series, which are capable of tasks like text generation, language translation, text completion, and text summarization. Decoder models are essential for applications requiring the generation of structured and ordered sequences of data, and their versatility makes them valuable in various NLP tasks and beyond."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Sequence-to-sequence Models**\n",
        "\n",
        "Sequence-to-sequence (Seq2Seq) models are a class of deep learning models used for various natural language processing (NLP) and machine learning tasks. They are designed to transform an input sequence into an output sequence, making them versatile for a wide range of applications. Seq2Seq models consist of two main components: an encoder and a decoder. Here's an overview of how they work:\n",
        "\n",
        "**Encoder**:\n",
        "1. **Input Encoding**: The encoder takes an input sequence (e.g., a sentence in one language) and processes it step by step. Each element of the input sequence (e.g., a word or token) is embedded into a continuous vector representation, capturing its semantic meaning.\n",
        "\n",
        "2. **Hidden States**: The encoder maintains a set of hidden states that evolve as it processes each element of the input sequence. These hidden states capture the contextual information and dependencies between the input elements.\n",
        "\n",
        "3. **Context Vector**: At the end of the encoding process, the encoder typically generates a single context vector or hidden state that summarizes the entire input sequence. This context vector is a high-level representation of the input and contains essential information.\n",
        "\n",
        "**Decoder**:\n",
        "1. **Initial State**: The decoder starts with an initial hidden state or context vector, which is often the context vector generated by the encoder.\n",
        "\n",
        "2. **Generating Output**: The decoder generates the output sequence (e.g., a translated sentence in another language) one element at a time. It predicts each element based on the input context and the elements it has generated so far.\n",
        "\n",
        "3. **Hidden States**: Similar to the encoder, the decoder maintains a set of hidden states that evolve as it generates each element of the output sequence. These hidden states capture the context and dependencies between the output elements.\n",
        "\n",
        "4. **Generating Output Tokens**: For each time step, the decoder uses its hidden state and the previously generated tokens to make predictions for the next token in the output sequence. This can involve using a softmax layer to select the most likely next token from a vocabulary.\n",
        "\n",
        "5. **Recurrent or Transformer Architectures**: Seq2Seq models can be built with recurrent neural networks (RNNs), long short-term memory networks (LSTMs), or transformer architectures. Transformer-based Seq2Seq models, like the Transformer and its variants, have gained prominence for their effectiveness in capturing long-range dependencies.\n",
        "\n",
        "**Training**:\n",
        "Seq2Seq models are typically trained using paired input-output sequences. The model's parameters are optimized to minimize the difference between the predicted output sequence and the target output sequence. Common loss functions for training Seq2Seq models include cross-entropy loss.\n",
        "\n",
        "**Applications**:\n",
        "Seq2Seq models have found application in various NLP tasks and beyond, including:\n",
        "- Machine Translation: Translating text from one language to another.\n",
        "- Text Summarization: Generating concise summaries of long documents.\n",
        "- Speech Recognition: Converting spoken language into text.\n",
        "- Text-to-Speech Synthesis: Generating natural-sounding speech from text.\n",
        "- Chatbots and Virtual Assistants: Responding to user queries and generating human-like text.\n",
        "\n",
        "Overall, Seq2Seq models are a versatile class of models that excel in tasks where sequences of data need to be transformed or generated, making them valuable in various real-world applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Bias and Limitation**\n",
        "\n",
        "Transformer models have revolutionized natural language processing (NLP) and have been applied successfully in a wide range of applications. However, they are not without their limitations and potential sources of bias. Here are some of the bias and limitations associated with transformer models:\n",
        "\n",
        "**1. Data Bias**:\n",
        "   - **Training Data Bias**: Transformer models learn from large datasets, which can contain biases present in the text data. These biases can include gender, race, and cultural biases. If not handled carefully, models can perpetuate and even amplify these biases in their outputs.\n",
        "   - **Source Data Bias**: Multilingual models trained on the web may inadvertently learn biased information from online sources, leading to biased translations or text generation.\n",
        "\n",
        "**2. Out-of-Distribution Data**:\n",
        "   - Transformer models, like other machine learning models, can struggle when faced with data that significantly differs from their training data. They might produce inaccurate or biased outputs when presented with out-of-distribution input.\n",
        "\n",
        "**3. Fairness and Bias Mitigation**:\n",
        "   - Ensuring fairness in transformer models is a complex challenge. Approaches to mitigate bias include debiasing training data, modifying model architectures, and incorporating fairness metrics.\n",
        "\n",
        "**4. Misinformation**:\n",
        "   - Transformers can generate text that is factually incorrect or misleading if the training data contains inaccuracies. Models may generate plausible-sounding but false information.\n",
        "\n",
        "**5. Generating Offensive Content**:\n",
        "   - Transformer models can generate content that is offensive, harmful, or inappropriate, reflecting the biases present in the training data. This has led to concerns about the ethical use of these models.\n",
        "\n",
        "**6. Contextual Bias**:\n",
        "   - Transformers may exhibit contextual bias, where their output is influenced by the context provided in the input. The same prompt may yield different responses based on minor changes in phrasing or context.\n",
        "\n",
        "**7. Understanding Causality**:\n",
        "   - Transformers excel at capturing correlations in data but may not understand causality. They can generate misleading or spurious correlations in their outputs.\n",
        "\n",
        "**8. Computational Resources**:\n",
        "   - Training and deploying transformer models can be computationally intensive and require substantial resources, making them less accessible for smaller organizations or research projects.\n",
        "\n",
        "**9. Long Sequences**:\n",
        "   - Transformers have a quadratic complexity with respect to the input sequence length, which can limit their ability to handle very long sequences efficiently.\n",
        "\n",
        "**10. Monolingual Focus**:\n",
        "   - While multilingual models exist, transformer models primarily focus on English and a few major languages. Less-resourced languages may receive less attention and produce less accurate results.\n",
        "\n",
        "**11. Lack of Explanation**:\n",
        "   - Transformers can be challenging to interpret, making it difficult to understand why a model produces a particular output. This lack of interpretability can be a limitation in applications where transparency is crucial.\n",
        "\n",
        "Addressing these limitations and biases is an ongoing area of research and development in NLP. Researchers and practitioners are working to create more fair and unbiased models, improve the explainability of transformer models, and enhance their robustness to out-of-distribution data. Ethical considerations and responsible AI practices are essential when working with transformer models to minimize bias and limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd021d03d9e340d98fcfc4b7ffde76f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9e887db079648dc9f64122dc99203c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5113e895c7e34b4bbfc41e4625e102f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99adc7dc30584c608c45f1319292d8a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a72ca991d42c4e96a008cf03b1fef391",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
            "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "result = unmasker(\"This man works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"This woman works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter/waitress). The others are work occupations usually associated with one specific gender — and yes, prostitute ended up in the top 5 possibilities the model associates with “woman” and “work.” This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet, but rather using apparently neutral data (it’s trained on the English Wikipedia and BookCorpus datasets).\n",
        "\n",
        "When you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data won’t make this intrinsic bias disappear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Summary**\n",
        "\n",
        "Transformer models are a groundbreaking type of deep learning architecture for natural language processing (NLP) and various machine learning tasks. They operate through a self-attention mechanism, which allows them to capture complex relationships in sequential data. Key components and concepts associated with transformer models include:\n",
        "\n",
        "1. **Input Embedding**: Transformer models convert input sequences into continuous vector representations, capturing semantic meanings.\n",
        "\n",
        "2. **Positional Encoding**: To account for the order of tokens in a sequence, transformers add positional encodings to the token embeddings.\n",
        "\n",
        "3. **Self-Attention Mechanism**: The core of the transformer architecture, self-attention enables each token to consider relationships and dependencies with all other tokens in the sequence.\n",
        "\n",
        "4. **Encoder**: The encoder processes the input sequence, generating context representations and a context vector that summarizes the entire input.\n",
        "\n",
        "5. **Decoder**: For sequence-to-sequence tasks, a decoder processes the context vector and generates an output sequence one element at a time.\n",
        "\n",
        "6. **Multiple Layers**: Transformers consist of multiple encoder-decoder layers, each featuring self-attention mechanisms and feedforward networks.\n",
        "\n",
        "7. **Training and Fine-Tuning**: Transformers are trained on large datasets using objectives like language modeling, and they can be fine-tuned on specific tasks with labeled data.\n",
        "\n",
        "8. **Applications**: Transformer models have excelled in a wide range of NLP tasks, including machine translation, text classification, question answering, text generation, and more.\n",
        "\n",
        "However, transformer models have certain limitations and sources of bias:\n",
        "- Data bias can lead to biased language generation and translation.\n",
        "- Handling out-of-distribution data can be challenging.\n",
        "- Efforts are needed to ensure fairness, reduce misinformation, and prevent the generation of offensive content.\n",
        "- Transformers may not understand causality well and can exhibit contextual bias.\n",
        "- Training and deploying transformers require substantial computational resources.\n",
        "- Interpretability and transparency can be challenging with transformers.\n",
        "\n",
        "Efforts to address these limitations and biases are ongoing in the field of NLP. Responsible AI practices and ethical considerations are crucial when working with transformer models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model_inputs = torch.tensor(encoded_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = model(model_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db024d8af98a4f02af5e3dc56c996101",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49a73419e2fc4aecbe733e48c4ab25ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb5db72ef8184f5fb92e1b807f5df3df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "too many indices for tensor of dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/Gen-AI-Open-AI/NLP.ipynb Cell 67\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bturbo-space-goggles-57qv554xxj6hvp64/workspaces/Gen-AI-Open-AI/NLP.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(ids)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bturbo-space-goggles-57qv554xxj6hvp64/workspaces/Gen-AI-Open-AI/NLP.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# This line will fail.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bturbo-space-goggles-57qv554xxj6hvp64/workspaces/Gen-AI-Open-AI/NLP.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m model(input_ids)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:789\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    787\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 789\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    790\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    791\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    792\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    793\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    794\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    795\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    796\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    797\u001b[0m )\n\u001b[1;32m    798\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    799\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:592\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m    593\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39msize()\n\u001b[1;32m    594\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/modeling_utils.py:3941\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   3938\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   3940\u001b[0m \u001b[39m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[0;32m-> 3941\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id \u001b[39min\u001b[39;00m input_ids[:, [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m]]:\n\u001b[1;32m   3942\u001b[0m     warn_string \u001b[39m=\u001b[39m (\n\u001b[1;32m   3943\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3944\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3945\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3946\u001b[0m     )\n\u001b[1;32m   3948\u001b[0m     \u001b[39m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[1;32m   3949\u001b[0m     \u001b[39m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102]])\n"
          ]
        }
      ],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.0.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "001a44e0b7bd4bb296ac1136869ed490": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03ac5c5626c9471594442aa1e87c07fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99816fb479da4bad90780f5f44889dbd",
              "IPY_MODEL_677b1187ee314498b4f0f5b090601b7a",
              "IPY_MODEL_eaae09be1ecf404393e1a503575e6326"
            ],
            "layout": "IPY_MODEL_218181680b8c43b6a561c3498f3ab557"
          }
        },
        "13bcac74bcbb46219996078a3daeaf36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f56454e07334007bca84f7a957b856f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21178e9225de463494fc2d73b58f2c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "218181680b8c43b6a561c3498f3ab557": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e579ee522fe4e479433f106b51f21d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5daa8ad3c3f54445a7a1af0b024894e7",
              "IPY_MODEL_d69c9cacdc5f4b1583826f2099cd00d9",
              "IPY_MODEL_2fe1d42694eb4d26b6194d17da0cc187"
            ],
            "layout": "IPY_MODEL_001a44e0b7bd4bb296ac1136869ed490"
          }
        },
        "2fa580965e5d47d9a248f1fa1fdee32e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fe1d42694eb4d26b6194d17da0cc187": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fa580965e5d47d9a248f1fa1fdee32e",
            "placeholder": "​",
            "style": "IPY_MODEL_52394f90dd9f475786ec753deb975f89",
            "value": " 232k/232k [00:00&lt;00:00, 2.65MB/s]"
          }
        },
        "35a06f5db3ef4972a7abd1897e7fb5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f69354c92faa4e5bb4a7d1b6ca6071cd",
            "placeholder": "​",
            "style": "IPY_MODEL_7a49585efaea4cf3ba9797ab5eebb5e5",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "44f9fb468f974cafb4f50253b749c042": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dad16a8c4064fe6890dd93b9f7b6db1",
            "placeholder": "​",
            "style": "IPY_MODEL_87906cf526d84717ab9d8e6547336257",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "4822d15150ac4cb9b822b626d12093ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ad15a761825490d982e71ca2df0dff1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52394f90dd9f475786ec753deb975f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5af149d50e2648479aab67ec2dd5ff32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5daa8ad3c3f54445a7a1af0b024894e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a967c9408e814554a0953b7a1ba25c06",
            "placeholder": "​",
            "style": "IPY_MODEL_96738277fda147268994e8224da1d623",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "677b1187ee314498b4f0f5b090601b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eeb57da42cd43efa4ff2eaed0b83355",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da3a5cf5d12c4e4794150101f05178d1",
            "value": 629
          }
        },
        "70a8e863503d4f008edf382a63d4acf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "797b4a33d9d74282a110b8b4b3e323fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a49585efaea4cf3ba9797ab5eebb5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dad16a8c4064fe6890dd93b9f7b6db1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84776f27dc8b429db6fa186ce1e7661b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35a06f5db3ef4972a7abd1897e7fb5f2",
              "IPY_MODEL_c2d2da4588e64889a3db2aff47e2be20",
              "IPY_MODEL_de9bd01014a94418a64ba813c468e524"
            ],
            "layout": "IPY_MODEL_a20b001d5f0446f7a7be7f6506ef7d88"
          }
        },
        "87906cf526d84717ab9d8e6547336257": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8eeb57da42cd43efa4ff2eaed0b83355": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96738277fda147268994e8224da1d623": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99816fb479da4bad90780f5f44889dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ad15a761825490d982e71ca2df0dff1",
            "placeholder": "​",
            "style": "IPY_MODEL_dc87d82eb7e94989ae462328229a1b83",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "9ce146470f474a429af294d52563a22c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a20b001d5f0446f7a7be7f6506ef7d88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a967c9408e814554a0953b7a1ba25c06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa95a903390343cba6653c90da982bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21178e9225de463494fc2d73b58f2c1a",
            "max": 267832558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13bcac74bcbb46219996078a3daeaf36",
            "value": 267832558
          }
        },
        "c2d2da4588e64889a3db2aff47e2be20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c82717ff51954d049f545421b7ada1c9",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ce146470f474a429af294d52563a22c",
            "value": 48
          }
        },
        "c5705bd1c82f401cb9675c0d4cb94045": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaa02bf7dd7a4010af43dd95c83e2b76",
            "placeholder": "​",
            "style": "IPY_MODEL_e65bb27895e94077a28cea035dbd399e",
            "value": " 268M/268M [00:02&lt;00:00, 79.2MB/s]"
          }
        },
        "c7e9948dbed14620973c9cc5cf2a265c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c82717ff51954d049f545421b7ada1c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c86f3c1b7c64407abadf78ad3b1056f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44f9fb468f974cafb4f50253b749c042",
              "IPY_MODEL_aa95a903390343cba6653c90da982bf6",
              "IPY_MODEL_c5705bd1c82f401cb9675c0d4cb94045"
            ],
            "layout": "IPY_MODEL_c7e9948dbed14620973c9cc5cf2a265c"
          }
        },
        "d69c9cacdc5f4b1583826f2099cd00d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f101fee70cc746088069b18f9246e438",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70a8e863503d4f008edf382a63d4acf3",
            "value": 231508
          }
        },
        "da3a5cf5d12c4e4794150101f05178d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc87d82eb7e94989ae462328229a1b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de9bd01014a94418a64ba813c468e524": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5af149d50e2648479aab67ec2dd5ff32",
            "placeholder": "​",
            "style": "IPY_MODEL_797b4a33d9d74282a110b8b4b3e323fe",
            "value": " 48.0/48.0 [00:00&lt;00:00, 644B/s]"
          }
        },
        "e65bb27895e94077a28cea035dbd399e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaa02bf7dd7a4010af43dd95c83e2b76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaae09be1ecf404393e1a503575e6326": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4822d15150ac4cb9b822b626d12093ce",
            "placeholder": "​",
            "style": "IPY_MODEL_1f56454e07334007bca84f7a957b856f",
            "value": " 629/629 [00:00&lt;00:00, 28.8kB/s]"
          }
        },
        "f101fee70cc746088069b18f9246e438": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f69354c92faa4e5bb4a7d1b6ca6071cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
